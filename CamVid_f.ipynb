{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvcCjMe0zeYx"
   },
   "source": [
    "# Project on the CamVid dataset\n",
    "\n",
    "Implementation of different semantic segmentation networks and their comparison on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34417,
     "status": "ok",
     "timestamp": 1762889760174,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "Iw6oqLFlDLpw",
    "outputId": "9aebb9c5-651a-487c-8394-9bedfc952cca"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuMcf4yIDUGD"
   },
   "source": [
    "### Imports and deterministic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21270,
     "status": "ok",
     "timestamp": 1762889781446,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "KH4fo3cxDXt2",
    "outputId": "c565a975-00a3-49ed-f237-b0b39adee2a1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import PIL\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchsummary import summary\n",
    "import tqdm\n",
    "!pip install wandb\n",
    "import wandb\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1762889781535,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "5tB4RaxyDaCv",
    "outputId": "9d0d4e59-faa6-4b9f-aacd-ea9da33a2e2d"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")\n",
    "def set_seed(seed: int = 42):\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnpLiZvMDh9W"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PV_QdLbDjR6"
   },
   "source": [
    "### Loading dataset and obtaining new classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19406,
     "status": "ok",
     "timestamp": 1762889800944,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "xEFHu3iLDpN6",
    "outputId": "281b0092-9fe6-4cac-8eeb-cb47f8e889bf"
   },
   "outputs": [],
   "source": [
    "src_zip = \"/content/drive/MyDrive/CamVid/CamVid.zip\"\n",
    "dst_zip = \"/content/CamVid\"\n",
    "\n",
    "with zipfile.ZipFile(src_zip, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(dst_zip)\n",
    "\n",
    "TEST_PATH = \"/content/CamVid/CamVid/test\"\n",
    "TRAIN_PATH = \"/content/CamVid/CamVid/train\"\n",
    "VALID_PATH = \"/content/CamVid/CamVid/val\"\n",
    "LABEL_TEST_PATH = \"/content/CamVid/CamVid/test_labels\"\n",
    "LABEL_TRAIN_PATH = \"/content/CamVid/CamVid/train_labels\"\n",
    "LABEL_VALID_PATH = \"/content/CamVid/CamVid/val_labels\"\n",
    "\n",
    "print(f\"There are {len([img for img in os.listdir(TRAIN_PATH)])} train images.\")\n",
    "print(f\"There are {len([img for img in os.listdir(TEST_PATH)])} test images.\")\n",
    "print(f\"There are {len([img for img in os.listdir(VALID_PATH)])} validation images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1762889800958,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "ANwmNE95DxWJ",
    "outputId": "a24a91c2-530d-4d25-82b1-f178a60a226a"
   },
   "outputs": [],
   "source": [
    "RGBLabel2LabelName = {\n",
    "    (128, 128, 128): \"Sky\",\n",
    "\n",
    "    (0, 128, 64): \"Building\",\n",
    "    (128, 0, 0): \"Building\",\n",
    "    (64, 192, 0): \"Building\",\n",
    "    (64, 0, 64): \"Building\",\n",
    "    (192, 0, 128): \"Building\",\n",
    "\n",
    "    (192, 192, 128): \"Pole\",\n",
    "    (0, 0, 64): \"Pole\",\n",
    "\n",
    "    (128, 64, 128): \"Road\",\n",
    "    (128, 0, 192): \"Road\",\n",
    "    (192, 0, 64): \"Road\",\n",
    "\n",
    "    (0, 0, 192): \"Sidewalk\",\n",
    "    (64, 192, 128): \"Sidewalk\",\n",
    "    (128, 128, 192): \"Sidewalk\",\n",
    "\n",
    "    (128, 128, 0): \"Tree\",\n",
    "    (192, 192, 0): \"Tree\",\n",
    "\n",
    "    (192, 128, 128): \"SignSymbol\",\n",
    "    (128, 128, 64): \"SignSymbol\",\n",
    "    (0, 64, 64): \"SignSymbol\",\n",
    "\n",
    "    (64, 64, 128): \"Fence\",\n",
    "\n",
    "    (64, 0, 128): \"Car\",\n",
    "    (64, 128, 192): \"Car\",\n",
    "    (192, 128, 192): \"Car\",\n",
    "    (192, 64, 128): \"Car\",\n",
    "    (128, 64, 64): \"Car\",\n",
    "\n",
    "    (64, 64, 0): \"Pedestrian\",\n",
    "    (192, 128, 64): \"Pedestrian\",\n",
    "    (64, 0, 192): \"Pedestrian\",\n",
    "    (64, 128, 64): \"Pedestrian\",\n",
    "\n",
    "    (0, 128, 192): \"Bicyclist\",\n",
    "    (192, 0, 192): \"Bicyclist\",\n",
    "\n",
    "    (0, 0, 0): \"Void\"\n",
    "}\n",
    "\n",
    "CAMVID_CLASSES = ['Sky',\n",
    "                  'Building',\n",
    "                  'Pole',\n",
    "                  'Road',\n",
    "                  'Sidewalk',\n",
    "                  'Tree',\n",
    "                  'SignSymbol',\n",
    "                  'Fence',\n",
    "                  'Car',\n",
    "                  'Pedestrian',\n",
    "                  'Bicyclist',\n",
    "                  'Void']\n",
    "Class2LabelId = {}\n",
    "\n",
    "for i, v in enumerate(CAMVID_CLASSES):\n",
    "    Class2LabelId[v] = i\n",
    "\n",
    "Class2LabelId['Void'] = 255\n",
    "NUM_CLASSES = len(CAMVID_CLASSES)\n",
    "print(NUM_CLASSES)\n",
    "\n",
    "CAMVID_CLASS_COLORS = [\n",
    "    (128, 128, 128),  # Sky\n",
    "    (128, 0, 0),      # Building\n",
    "    (192, 192, 128),  # Pole\n",
    "    (128, 64, 128),   # Road\n",
    "    (0, 0, 192),      # Sidewalk\n",
    "    (128, 128, 0),    # Tree\n",
    "    (192, 128, 128),  # SignSymbol\n",
    "    (64, 64, 128),    # Fence\n",
    "    (64, 0, 128),     # Car\n",
    "    (64, 64, 0),      # Pedestrian\n",
    "    (0, 128, 192),    # Bicyclist\n",
    "    (0, 0, 0),        # Void / background\n",
    "]\n",
    "\n",
    "# Build a dictionary {class_id: RGB tuple}\n",
    "COLORMAP = {i: color for i, color in enumerate(CAMVID_CLASS_COLORS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSoLPaPiD8Pz"
   },
   "outputs": [],
   "source": [
    "def convert32to11(src_label_dir, dst_label_dir):\n",
    "\n",
    "  if not os.path.exists(dst_label_dir):\n",
    "    os.makedirs(dst_label_dir)\n",
    "  else:\n",
    "    print(f\"{dst_label_dir} already exists\")\n",
    "    return\n",
    "\n",
    "  img_names = sorted([img_name for img_name in os.listdir(src_label_dir)])\n",
    "\n",
    "  for img in img_names:\n",
    "\n",
    "    print(os.path.join(src_label_dir, img))\n",
    "    image = PIL.Image.open(os.path.join(src_label_dir, img))\n",
    "    np_img = np.array(image)\n",
    "    ret_img = np.ones(np_img.shape[:2], dtype=np.uint8) * 255\n",
    "    w, h = np_img.shape[:2]\n",
    "\n",
    "    for x in range(w):\n",
    "      for y in range(h):\n",
    "        if tuple(np_img[x,y]) in RGBLabel2LabelName:\n",
    "          ret_img[x,y] = Class2LabelId[RGBLabel2LabelName[tuple(np_img[x,y])]]\n",
    "        else:\n",
    "          ret_img[x,y] = 255\n",
    "\n",
    "    ret_img = PIL.Image.fromarray(ret_img)\n",
    "    ret_img.save(os.path.join(dst_label_dir, img))\n",
    "    print(f\"Converted {img}\")\n",
    "  print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubZTfbofEBtM"
   },
   "outputs": [],
   "source": [
    "DRIVE_BASE = \"/content/drive/MyDrive/CamVid/CamVid_Labels\"\n",
    "# convert32to11(LABEL_TEST_PATH, f\"{DRIVE_BASE}/test_labels_11\")\n",
    "# convert32to11(LABEL_TRAIN_PATH, f\"{DRIVE_BASE}/train_labels_11\")\n",
    "# convert32to11(LABEL_VALID_PATH, f\"{DRIVE_BASE}/valid_labels_11\")\n",
    "LABEL_TRAIN_PATH = \"/content/drive/MyDrive/CamVid/CamVid_Labels/train_labels_11\"\n",
    "LABEL_TEST_PATH = \"/content/drive/MyDrive/CamVid/CamVid_Labels/test_labels_11\"\n",
    "LABEL_VALID_PATH = \"/content/drive/MyDrive/CamVid/CamVid_Labels/valid_labels_11\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5ge4UoTG9Rj"
   },
   "source": [
    "### Add train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1762889801294,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "o93tOwBjHAK-",
    "outputId": "83133f34-ca2d-4ac8-d35e-99f1213ab4d5"
   },
   "outputs": [],
   "source": [
    "num_to_move = 132  # number of images to move\n",
    "\n",
    "# Directories\n",
    "images_root = \"/content/CamVid/CamVid\"\n",
    "labels_root = \"/content/drive/My Drive/CamVid/CamVid_Labels\"\n",
    "\n",
    "# Subdirectories\n",
    "train_img_dir = os.path.join(images_root, \"train\")\n",
    "test_img_dir = os.path.join(images_root, \"test\")\n",
    "train_label_dir = os.path.join(labels_root, \"train_labels_11_v2\")\n",
    "test_label_dir = os.path.join(labels_root, \"test_labels_11_v2\")\n",
    "\n",
    "train_2_dir = \"/content/CamVid/CamVid/train_2\"\n",
    "test_2_dir = \"/content/CamVid/CamVid/test_2\"\n",
    "\n",
    "for src, dst in [(train_img_dir, train_2_dir), (test_img_dir, test_2_dir)]:\n",
    "    os.makedirs(dst, exist_ok=True)\n",
    "    for file in os.listdir(src):\n",
    "        src_path = os.path.join(src, file)\n",
    "        dst_path = os.path.join(dst, file)\n",
    "        if os.path.isfile(src_path):\n",
    "            shutil.copy(src_path, dst_path)\n",
    "print(\"Files in destination:\", len(os.listdir(train_2_dir)))\n",
    "print(\"File in source: \", len(os.listdir(test_2_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-Qctzz4HNGM"
   },
   "outputs": [],
   "source": [
    "# EXECUTE ONLY FIRST TIME\n",
    "\n",
    "# save_list_path = \"/content/drive/My Drive/CamVid/CamVid_Labels/masks_to_move.txt\"\n",
    "\n",
    "# test_masks = [f for f in os.listdir(test_label_dir) if f.endswith(\"png\")]\n",
    "# print(f\"Found {len(test_masks)} masks in test_labels_11_v2\")\n",
    "\n",
    "# selected_masks = random.sample(test_masks, min(num_to_move, len(test_masks)))\n",
    "# with open(save_list_path, \"w\") as f:\n",
    "#     for name in selected_masks:\n",
    "#         f.write(name + \"\\n\")\n",
    "\n",
    "# print(f\"Saved list of {len(selected_masks)} randomly chosen masks to: {save_list_path}\")\n",
    "# print(\"Example of first 5 chosen files:\", selected_masks[:5])\n",
    "\n",
    "# with open(\"/content/drive/My Drive/CamVid/CamVid_Labels/masks_to_move.txt\", 'r') as f:\n",
    "#     filenames = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# for name in filenames:\n",
    "#     src_path = os.path.join(test_label_dir, name)\n",
    "#     dst_path = os.path.join(train_label_dir, name)\n",
    "\n",
    "#     if os.path.exists(src_path):\n",
    "#         shutil.move(src_path, dst_path)\n",
    "#     else:\n",
    "#         print(f\"⚠️ File not found: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1762889802143,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "dFeNgI11HTnv",
    "outputId": "b9784c09-8232-4ff1-cad8-96e7416d8ae2"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(\"/content/drive/My Drive/CamVid/CamVid_Labels/masks_to_move.txt\", 'r') as f:\n",
    "    raw_names = [os.path.splitext(line.strip())[0] for line in f if line.strip()]\n",
    "\n",
    "def clean_name(name):\n",
    "    return re.split(r'_L|\\s|\\(', name)[0].lower()\n",
    "\n",
    "base_names = [clean_name(n) for n in raw_names]\n",
    "print(base_names[:5])\n",
    "\n",
    "# Ensure destination exists\n",
    "os.makedirs(train_2_dir, exist_ok=True)\n",
    "\n",
    "# Loop through source directory and move matches\n",
    "moved = 0\n",
    "for filename in os.listdir(test_2_dir):\n",
    "    name_no_ext, ext = os.path.splitext(filename)\n",
    "    if name_no_ext.lower() in base_names:\n",
    "        shutil.move(os.path.join(test_2_dir, filename), os.path.join(train_2_dir, filename))\n",
    "        moved += 1\n",
    "\n",
    "print(f\"Moved {moved} files from {test_2_dir} to {train_2_dir}\")\n",
    "\n",
    "NEW_TRAIN_LABELS = \"/content/drive/My Drive/CamVid/CamVid_Labels/train_labels_11_v2\"\n",
    "NEW_TEST_LABELS = \"/content/drive/My Drive/CamVid/CamVid_Labels/test_labels_11_v2\"\n",
    "NEW_TRAIN_PATH = \"/content/CamVid/CamVid/train_2\"\n",
    "NEW_TEST_PATH = \"/content/CamVid/CamVid/test_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KLwhWkEEISK"
   },
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVwGYh-9EJ_P"
   },
   "outputs": [],
   "source": [
    "class CamVidDataset(Dataset):\n",
    "\n",
    "  def __init__(self, img_dir, mask_dir, width, height, transforms=None, color_transforms=None):\n",
    "\n",
    "    self.img_dir = img_dir\n",
    "    self.mask_dir = mask_dir\n",
    "\n",
    "    self.images = sorted([img_name for img_name in os.listdir(img_dir)])\n",
    "    self.masks = sorted([mask_name for mask_name in os.listdir(mask_dir)])\n",
    "\n",
    "    self.transforms = transforms\n",
    "    self.color_transforms = color_transforms\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.images)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "\n",
    "    img_name = self.images[idx]\n",
    "    mask_name = self.masks[idx]\n",
    "\n",
    "    img = cv2.imread(os.path.join(self.img_dir, img_name))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    mask = cv2.imread(os.path.join(self.mask_dir, mask_name), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    img_resized = cv2.resize(img, (self.width, self.height))\n",
    "    mask_resized = cv2.resize(mask, (self.width, self.height), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    img_resized = torch.from_numpy(img_resized).permute(2, 0, 1).float() / 255.0\n",
    "    mask_resized = torch.from_numpy(mask_resized).long()\n",
    "\n",
    "    mask_resized = mask_resized.unsqueeze(0)\n",
    "\n",
    "    if self.transforms:\n",
    "      img_tv = torchvision.tv_tensors.Image(img_resized)\n",
    "      mask_tv = torchvision.tv_tensors.Mask(mask_resized)\n",
    "      img_resized, mask_resized = self.transforms(img_tv, mask_tv)\n",
    "\n",
    "    if self.color_transforms:\n",
    "      img_resized = self.transforms(img_resized)\n",
    "\n",
    "    return img_resized, mask_resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64sF7OIZFbai"
   },
   "source": [
    "### Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuQUsJDoFdkk"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def get_train_transform():\n",
    "\n",
    "  return T.Compose([\n",
    "      T.ToDtype(torch.uint8, scale=True),\n",
    "      T.RandomHorizontalFlip(p=0.5),\n",
    "      T.RandomRotation(degrees=30),\n",
    "      T.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "      T.ToDtype(torch.float32, scale=True),\n",
    "      T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "  ])\n",
    "\n",
    "def get_valid_transform():\n",
    "\n",
    "  return T.Compose([\n",
    "      T.ToDtype(torch.float32, scale=True),\n",
    "      T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mk49oVIqW_AQ"
   },
   "source": [
    "### Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37390,
     "status": "ok",
     "timestamp": 1762889839618,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "pjwJLMCmXA8y",
    "outputId": "fc9f4d44-7122-4ca7-c5fd-9f013d7b48b6"
   },
   "outputs": [],
   "source": [
    "t_d = CamVidDataset(NEW_TRAIN_PATH, NEW_TRAIN_LABELS, 520, 520, get_train_transform())\n",
    "t_loader = DataLoader(t_d, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "num_classes = 11\n",
    "class_counts = torch.zeros(num_classes)\n",
    "\n",
    "for _, targets in t_loader:  # targets: (B, H, W)\n",
    "    targets = torch.stack(targets, dim=0)  # (B, H, W) -> (B, 1, H, W)\n",
    "    targets = targets.squeeze(1)  # (B, 1, H, W) -> (B, H, W)\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        mask = (targets == cls) & (targets != 255)\n",
    "        class_counts[cls] += mask.sum().item()\n",
    "\n",
    "# Normalizing\n",
    "class_weights = 1.0 / (class_counts + 1e-6)  # avoid division by zero\n",
    "class_weights = class_weights / class_weights.max()\n",
    "\n",
    "# Median frequency balancing\n",
    "# frequencies = class_counts / class_counts.sum()\n",
    "# median_freq = torch.median(frequencies[frequencies > 0])\n",
    "# class_weights = median_freq / (frequencies + 1e-6)\n",
    "CLASS_WEIGHTS = torch.cat([class_weights, torch.tensor([0.0])])\n",
    "for i in range(len(CLASS_WEIGHTS)):\n",
    "  print(f\"Weights for class {CAMVID_CLASSES[i]} is : {CLASS_WEIGHTS[i]:.3f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 702,
     "status": "ok",
     "timestamp": 1762889840314,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "6m5ekdhBDojV",
    "outputId": "6616e57b-640e-4288-ed97-4ad053e7d1ed"
   },
   "outputs": [],
   "source": [
    "freq_np = class_counts.cpu().numpy()\n",
    "\n",
    "freq_np = (freq_np / freq_np.sum())\n",
    "\n",
    "# Sort for better readability\n",
    "sorted_idx = np.argsort(freq_np)[::-1]\n",
    "freq_np = freq_np[sorted_idx]\n",
    "CAMVID_CLASSES_SORTED = [CAMVID_CLASSES[i] for i in sorted_idx]\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define colors (optional but improves visual clarity)\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(freq_np)))\n",
    "\n",
    "# Bar chart\n",
    "bars = plt.bar(CAMVID_CLASSES_SORTED[:11], freq_np, color=colors, edgecolor='black', alpha=0.85)\n",
    "\n",
    "# Add percentage labels on top of each bar\n",
    "for bar, val in zip(bars, freq_np):\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width()/2,\n",
    "        val + 0.005,\n",
    "        f\"{val*100:.1f}%\",\n",
    "        ha='center', va='bottom', fontsize=10\n",
    "    )\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"CamVid Class Frequency Distribution\", fontsize=16, weight='bold')\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.xlabel(\"Classes\", fontsize=12)\n",
    "\n",
    "# Rotate x labels for readability\n",
    "plt.xticks(rotation=40, ha='right')\n",
    "\n",
    "# Optional grid for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-U850M5EFnL"
   },
   "source": [
    "## Visualization utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUSV5FQBEjFL"
   },
   "outputs": [],
   "source": [
    "def colorize_mask(mask):\n",
    "    \"\"\"Convert single-channel label mask to color RGB image.\"\"\"\n",
    "    mask = mask.squeeze(0)\n",
    "    mask = mask.cpu().numpy()\n",
    "    h, w = mask.shape\n",
    "    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for k, color in COLORMAP.items():\n",
    "        color_mask[mask == k] = color\n",
    "    return color_mask\n",
    "\n",
    "def denormalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    for t, m, s in zip(image, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return image\n",
    "\n",
    "def visualize(image, mask, predicted=None):\n",
    "\n",
    "  plt.figure(figsize=(16,12))\n",
    "\n",
    "  len = 2\n",
    "  if predicted is not None:\n",
    "    len = 3\n",
    "  denormalize(image)\n",
    "  plt.subplot(1, len, 1)\n",
    "  plt.imshow(image.permute(1, 2, 0))\n",
    "  plt.axis('off')\n",
    "  plt.title('Image')\n",
    "\n",
    "  plt.subplot(1, len, 2)\n",
    "  plt.imshow(colorize_mask(mask))\n",
    "  plt.axis('off')\n",
    "  plt.title('Ground Truth')\n",
    "  if predicted is not None:\n",
    "    plt.subplot(1, len, len)\n",
    "    plt.imshow(colorize_mask(predicted))\n",
    "    plt.axis('off')\n",
    "    plt.title('Prediction')\n",
    "  plt.show()\n",
    "\n",
    "def plot_curves(train_loss, val_loss, val_metric):\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # ---- Subplot 1: Training & Validation Loss ----\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, label='Train Loss', linewidth=2)\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # ---- Subplot 2: Validation Metric ----\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, val_metric, color='green', label='Validation Metric', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean IoU')\n",
    "    plt.title('Validation Metric')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_trainable_params(model):\n",
    "\n",
    "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "  total_params = sum(p.numel() for p in model.parameters())\n",
    "  print(f\"Trainable parameters: {trainable_params:,} / {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "output_embedded_package_id": "1e-Lsomj9lAZweCjVzuNPEuKlAv2YeAYl"
    },
    "executionInfo": {
     "elapsed": 8909,
     "status": "ok",
     "timestamp": 1762889849440,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "4GQ1s19JGutA",
    "outputId": "b2f8c85b-248d-4a12-ab38-f01483e888d3"
   },
   "outputs": [],
   "source": [
    "data = CamVidDataset(NEW_TRAIN_PATH, NEW_TRAIN_LABELS, 520, 520, get_valid_transform())\n",
    "data_transf = CamVidDataset(NEW_TRAIN_PATH, NEW_TRAIN_LABELS, 520, 520, get_train_transform())\n",
    "\n",
    "img, mask = data[0]\n",
    "img_transf, mask_transf = data_transf[0]\n",
    "\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "\n",
    "visualize(img, mask)\n",
    "visualize(data[5][0], data[5][1])\n",
    "visualize(data[20][0], data[20][1])\n",
    "visualize(data[100][0], data[100][1])\n",
    "visualize(data[300][0], data[300][1])\n",
    "visualize(img_transf, mask_transf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wK9gQrPqH66X"
   },
   "source": [
    "## Evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvXbStJ4H8wl"
   },
   "source": [
    "I will evaluate using mean Intersection over Union and I will also provide a pixel accuracy measure. Moreover, here, I implement also a function that completely evaluates the model on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLormgSdIVX2"
   },
   "outputs": [],
   "source": [
    "def multiclass_iou(preds: torch.Tensor, targets: torch.Tensor, num_classes: int = 11, void_label: int = 255):\n",
    "\n",
    "    # Convert logits to predicted class IDs\n",
    "    pred_classes = preds.argmax(dim=1)  # (B, H, W)\n",
    "\n",
    "    # Mask out void pixels\n",
    "    valid_mask = targets != void_label\n",
    "    pred_classes = pred_classes[valid_mask]\n",
    "    targets = targets[valid_mask]\n",
    "\n",
    "    # Flatten everything\n",
    "    pred_classes = pred_classes.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    intersection = torch.zeros(num_classes, dtype=torch.float32, device=preds.device)\n",
    "    union = torch.zeros(num_classes, dtype=torch.float32, device=preds.device)\n",
    "\n",
    "    # Compute per-class intersection and union\n",
    "    for cls in range(num_classes):\n",
    "        pred_mask = pred_classes == cls\n",
    "        target_mask = targets == cls\n",
    "\n",
    "        inter = (pred_mask & target_mask).sum()\n",
    "        u = pred_mask.sum() + target_mask.sum() - inter\n",
    "\n",
    "        intersection[cls] = inter\n",
    "        union[cls] = u\n",
    "\n",
    "    # IoU per class\n",
    "    iou_per_class = intersection / (union + 1e-6)\n",
    "    iou_per_class[union == 0] = float('nan')  # skip absent classes\n",
    "\n",
    "    # Mean IoU over valid (non-NaN) classes\n",
    "    mean_iou = torch.nanmean(iou_per_class)\n",
    "\n",
    "    return iou_per_class, mean_iou\n",
    "\n",
    "\n",
    "def compute_pixel_accuracy_multiclass(model, dataloader, device='cuda', pretrained=False):\n",
    "\n",
    "    model.eval()\n",
    "    correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images = torch.stack(images, dim=0).to(device)\n",
    "            masks = torch.stack(masks, dim=0).to(device)\n",
    "\n",
    "            # Ensure masks have shape [B, H, W]\n",
    "            if masks.ndim == 4 and masks.shape[1] == 1:\n",
    "                masks = masks.squeeze(1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            if pretrained:\n",
    "              outputs = outputs['out']\n",
    "\n",
    "            # Predicted class per pixel\n",
    "            preds = torch.argmax(outputs, dim=1)  # [B, H, W]\n",
    "\n",
    "            # Count correct pixels\n",
    "            correct_pixels += (preds == masks).sum().item()\n",
    "            total_pixels += masks.numel()\n",
    "\n",
    "    pixel_acc = correct_pixels / total_pixels\n",
    "    return pixel_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PM75pcAEIwnV"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, test_loader, device, num_classes=11, void_label=255, pretrained=False, class_weights=None):\n",
    "    model.eval()\n",
    "\n",
    "    total_intersection = torch.zeros(num_classes, dtype=torch.float32, device=device)\n",
    "    total_union = torch.zeros(num_classes, dtype=torch.float32, device=device)\n",
    "\n",
    "    correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "\n",
    "    for images, targets in tqdm.tqdm(test_loader, desc=\"Evaluating\", leave=False):\n",
    "        images = torch.stack(images, dim=0).to(device)\n",
    "        targets = torch.stack(targets, dim=0).to(device).squeeze(1)\n",
    "\n",
    "        # Forward pass\n",
    "        preds = model(images)  # (B, C, H, W)\n",
    "\n",
    "        if pretrained:\n",
    "          preds = preds['out']\n",
    "\n",
    "        # Compute per-batch IoU stats\n",
    "        pred_classes = preds.argmax(dim=1)\n",
    "        correct_pixels += (pred_classes == targets).sum().item()\n",
    "        total_pixels += targets.numel()\n",
    "\n",
    "        valid_mask = targets != void_label\n",
    "        valid_mask = valid_mask\n",
    "        pred_classes = pred_classes[valid_mask]\n",
    "        targets_flat = targets[valid_mask]\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            pred_mask = pred_classes == cls\n",
    "            target_mask = targets_flat == cls\n",
    "\n",
    "            inter = (pred_mask & target_mask).sum()\n",
    "            u = pred_mask.sum() + target_mask.sum() - inter\n",
    "\n",
    "            total_intersection[cls] += inter\n",
    "            total_union[cls] += u\n",
    "\n",
    "    # Compute final IoU\n",
    "    iou_per_class = total_intersection / (total_union + 1e-6)\n",
    "    iou_per_class[total_union == 0] = float('nan')\n",
    "\n",
    "    if class_weights is None:\n",
    "      mean_iou = torch.nanmean(iou_per_class)\n",
    "    else:\n",
    "      weights = class_weights.to(device).clone()\n",
    "      # weigths = weights[:num_classes]\n",
    "      weights[torch.isnan(iou_per_class)] = 0\n",
    "      weights = weights / weights.sum()\n",
    "      mean_iou = torch.nanmean(iou_per_class * weights)\n",
    "\n",
    "\n",
    "    print(\"\\nPer-class IoU:\")\n",
    "    for i, iou in enumerate(iou_per_class):\n",
    "        print(f\"  Class {i} - {CAMVID_CLASSES[i]}: {iou.item():.4f}\")\n",
    "    print(f\"\\nMean IoU: {mean_iou.item():.4f}\")\n",
    "    print(f\"\\nPixel Accuracy: {correct_pixels / total_pixels:.4f}\")\n",
    "\n",
    "    return iou_per_class, mean_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrsmvvYJ8S7M"
   },
   "source": [
    "## Focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4_S0rbX8WDa"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha=None, gamma=2, ignore_index=255, device=device):\n",
    "      super().__init__()\n",
    "      self.alpha = alpha.to(device)\n",
    "      self.gamma = gamma\n",
    "      self.ignore_index = ignore_index\n",
    "      self.device = device\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "\n",
    "      log_prob = F.log_softmax(inputs, dim=1)\n",
    "      prob = torch.exp(log_prob)\n",
    "\n",
    "      targets[targets==255] = 11\n",
    "\n",
    "      idx = targets.unsqueeze(1)  #[B,1,H,W]\n",
    "      log_prob_true_classes = torch.gather(log_prob, dim=1, index=idx).squeeze(1) #[B,H,W]\n",
    "      prob_true_classes = torch.exp(log_prob_true_classes)\n",
    "      alpha_true_classes = self.alpha[targets] #[B,H,W]\n",
    "\n",
    "      focal = - (alpha_true_classes * (1 - prob_true_classes) ** self.gamma * log_prob_true_classes)\n",
    "\n",
    "      valid_mask = (targets != self.ignore_index)\n",
    "      focal = focal[valid_mask]\n",
    "\n",
    "      return focal.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guEpYdZYJ9r5"
   },
   "source": [
    "## Models Implementation\n",
    "\n",
    "I will implement different models and compare their performances on the dataset. I will try:\n",
    "- U-Net with traditional VGG backbone\n",
    "- U-Net with ResNets backbones\n",
    "- DeepLab V3+\n",
    "- Use the pretrained DeepLab V3 Pytorch provides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1762889849542,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "WXc-ggw6XF1E",
    "outputId": "d17bde7d-f3e9-4903-9645-f9d06ba9c04b"
   },
   "outputs": [],
   "source": [
    "!pip install thop\n",
    "\n",
    "from thop import profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OC7ZruV7KUjb"
   },
   "source": [
    "### U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1762889849551,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "5e_lxNAWKWCh",
    "outputId": "2558d601-be43-4efe-ffef-e26965917ae2"
   },
   "outputs": [],
   "source": [
    "class DoubleConvBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, in_channels, out_channels, dropout=0.0):\n",
    "\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "    self.use_dropout = dropout > 0\n",
    "    if self.use_dropout:\n",
    "      self.dropout = nn.Dropout2d(p=dropout)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "\n",
    "    x = self.conv1(inputs)\n",
    "    x = self.relu(x)\n",
    "    x = self.bn1(x)\n",
    "    if self.use_dropout:\n",
    "      x = self.dropout(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.bn2(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, in_channels, out_channels):\n",
    "    super().__init__()\n",
    "    self.cnv = DoubleConvBlock(in_channels, out_channels)\n",
    "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "\n",
    "    skip = self.cnv(inputs)\n",
    "    x = self.pool(skip)\n",
    "\n",
    "    return x, skip\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, in_channels, out_channels):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "    self.cnv = DoubleConvBlock(in_channels, out_channels)\n",
    "\n",
    "  def forward(self, inputs, skip):\n",
    "\n",
    "    x = self.up(inputs)\n",
    "    x = torch.cat([x, skip], axis=1)\n",
    "    x = self.cnv(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.encoder_1 = EncoderLayer(3, 64)\n",
    "    self.encoder_2 = EncoderLayer(64, 128)\n",
    "    self.encoder_3 = EncoderLayer(128, 256)\n",
    "    self.encoder_4 = EncoderLayer(256, 512)\n",
    "\n",
    "    # Bottlneck + added dropout\n",
    "    self.bottleneck = DoubleConvBlock(512, 1024, dropout=0.5)\n",
    "\n",
    "    self.decoder_1 = DecoderLayer(1024, 512)\n",
    "    self.decoder_2 = DecoderLayer(512, 256)\n",
    "    self.decoder_3 = DecoderLayer(256, 128)\n",
    "    self.decoder_4 = DecoderLayer(128, 64)\n",
    "\n",
    "    # Scorer\n",
    "    self.last = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "\n",
    "    x, skip_1 = self.encoder_1(inputs)\n",
    "    x, skip_2 = self.encoder_2(x)\n",
    "    x, skip_3 = self.encoder_3(x)\n",
    "    x, skip_4 = self.encoder_4(x)\n",
    "\n",
    "    x = self.bottleneck(x)\n",
    "\n",
    "    x = self.decoder_1(x, skip_4)\n",
    "    x = self.decoder_2(x, skip_3)\n",
    "    x = self.decoder_3(x, skip_2)\n",
    "    x = self.decoder_4(x, skip_1)\n",
    "\n",
    "    x = self.last(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "unet = UNet(NUM_CLASSES).to(device)\n",
    "# summary(unet, input_size=(3, 512, 512))\n",
    "\n",
    "input_tensor = torch.randn(1, 3, 512, 512).to(device)\n",
    "flops, params = profile(unet, inputs=(input_tensor, ))\n",
    "print(f\"FLOPs: {flops/1e9:.2f} GFLOPs\")\n",
    "print(f\"Params: {params/1e6:.2f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmmzTW5RKphU"
   },
   "source": [
    "### ResNet Unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jk9c2ZeZSJAM"
   },
   "source": [
    "#### ResNet-34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbUtPsivSYnS"
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, up_input_c, output_c, in_conv=None, dropout=0.1):\n",
    "\n",
    "    super().__init__()\n",
    "    if in_conv is None:\n",
    "      in_conv = up_input_c\n",
    "\n",
    "    self.up = nn.ConvTranspose2d(up_input_c, output_c, kernel_size=2, stride=2, padding=0)\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.Conv2d(in_conv, output_c, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(output_c),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Conv2d(output_c, output_c, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(output_c),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "  def forward(self, inputs, skip):\n",
    "    x = self.up(inputs)\n",
    "    x = torch.cat([x, skip], dim=1)\n",
    "    x = self.conv(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class UNetResNet34(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes, dropout=0.1, pretrained=True):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    if pretrained:\n",
    "      backbone = torchvision.models.resnet34(weights=torchvision.models.ResNet34_Weights.DEFAULT)\n",
    "    else:\n",
    "      backbone = torchvision.models.resnet34(weights=None)\n",
    "\n",
    "    # ENCODER\n",
    "\n",
    "    # Input: 3 x H x W ---> Output: 64 x H/2 x W/2\n",
    "    self.conv1 = backbone.conv1\n",
    "    self.bn1 = backbone.bn1\n",
    "    self.relu = backbone.relu\n",
    "\n",
    "    # Input: 64 x H/2 x W/2 --> Output: 64 x H/4 x W/4\n",
    "    self.maxpool = backbone.maxpool\n",
    "\n",
    "    # Input: 64 x H/4 x W/4 --> Output: 128 x H/4 x W/4\n",
    "    self.enc1 = backbone.layer1\n",
    "\n",
    "    #Input: 128 x H/4 x W/4 ---> Output: 256 x H/8 x W/8\n",
    "    self.enc2 = backbone.layer2\n",
    "\n",
    "    #Input: 256 x H/8 x W/8 ---> Output: 512 x H/16 x W/16\n",
    "    self.enc3 = backbone.layer3\n",
    "\n",
    "    # Input: 512 x H/16 x W/16 ---> Output: 512 x H/32 x W/32\n",
    "    self.enc4 = backbone.layer4\n",
    "\n",
    "    # Input: 512 x H/32 x W/32 ---> Output: 1024 x H/64 x W/64\n",
    "    self.bottleneck = nn.Sequential(\n",
    "        nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=dropout),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "    )\n",
    "\n",
    "    # DECODER\n",
    "    # Input: from upsampling 1024 x H/64 x W/64, from convolution 512 x H/32 x W/32\n",
    "    # Output: 512 x H/32 x W/32\n",
    "    self.decoder1 = DecoderBlock(1024, 512, dropout=dropout)\n",
    "\n",
    "    # Input: from upsampling 512 x H/32 x W/32, from convolution 256 x H/16 x W/16\n",
    "    # Output: 256 x H/16 x W/16\n",
    "    self.decoder2 = DecoderBlock(512, 256, dropout=dropout)\n",
    "\n",
    "    # Input: from upsampling 256 x H/16 x W/16, from convolution 128 x H/8 x W/8\n",
    "    # Output: 128 x H/8 x W/8\n",
    "    self.decoder3 = DecoderBlock(256, 128, dropout=dropout)\n",
    "\n",
    "    # Input: from upsampling 128 x H/8 x W/8, from convolution 64 x H/4 x W/4\n",
    "    # Output: 64 x H/4 x W/4\n",
    "    self.decoder4 = DecoderBlock(128, 64, dropout=dropout)\n",
    "\n",
    "    # Input: from upsampling 64 x H/4 x W/4, from convolution 64 x H/2 x W/2\n",
    "    # Output: 64 x H/2 x W/2\n",
    "    self.decoder5 = DecoderBlock(64, 64, dropout=dropout, in_conv=128)\n",
    "\n",
    "    # Input: 64 x H/2 x W/2, Output: n_classes x H x W\n",
    "    self.last = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=2, stride=2, padding=0),\n",
    "            nn.Conv2d(64, n_classes, kernel_size=3, padding=1)\n",
    "    )\n",
    "\n",
    "  def forward(self, inputs):\n",
    "\n",
    "    e0 = self.conv1(inputs)\n",
    "    e0 = self.bn1(e0)\n",
    "    e0 = self.relu(e0)\n",
    "    x = self.maxpool(e0)\n",
    "\n",
    "    # Encoder\n",
    "    enc1 = self.enc1(x)\n",
    "    enc2 = self.enc2(enc1)\n",
    "    enc3 = self.enc3(enc2)\n",
    "    enc4 = self.enc4(enc3)\n",
    "\n",
    "    bottleneck = self.bottleneck(enc4)\n",
    "\n",
    "    # Decoder\n",
    "    dec1 = self.decoder1(bottleneck, enc4)\n",
    "    dec2 = self.decoder2(dec1, enc3)\n",
    "    dec3 = self.decoder3(dec2, enc2)\n",
    "    dec4 = self.decoder4(dec3, enc1)\n",
    "    dec5 = self.decoder5(dec4, e0)\n",
    "    out = self.last(dec5)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1762889849591,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "H3zq4VJKShO-",
    "outputId": "ef618a4b-1036-45fd-b8fa-b0678e908a3f"
   },
   "outputs": [],
   "source": [
    "unet = UNetResNet34(NUM_CLASSES).to(device)\n",
    "# summary(unet, input_size=(3, 512, 512))\n",
    "input_tensor = torch.randn(1, 3, 512, 512).to(device)\n",
    "flops, params = profile(unet, inputs=(input_tensor, ))\n",
    "print(f\"FLOPs: {flops/1e9:.2f} GFLOPs\")\n",
    "print(f\"Params: {params/1e6:.2f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBkBIxgoSMhL"
   },
   "source": [
    "#### ResNet-50 and ResNet-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjfF38TbMDNo"
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, up_input_c, output_c, in_conv=None, dropout=0.1):\n",
    "\n",
    "    super().__init__()\n",
    "    if in_conv is None:\n",
    "      in_conv = up_input_c\n",
    "\n",
    "    self.up = nn.ConvTranspose2d(up_input_c, output_c, kernel_size=2, stride=2, padding=0)\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.Conv2d(in_conv, output_c, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(output_c),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Conv2d(output_c, output_c, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(output_c),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "  def forward(self, inputs, skip):\n",
    "    x = self.up(inputs)\n",
    "    x = torch.cat([x, skip], dim=1)\n",
    "    x = self.conv(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class UNetResNet(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes, backbone='resnet101', dropout=0.1):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    if backbone == 'resnet101':\n",
    "      backbone = torchvision.models.resnet101(weights=torchvision.models.ResNet101_Weights.DEFAULT)\n",
    "      filters = [64, 256, 512, 1024]\n",
    "    elif backbone == 'resnet50':\n",
    "      backbone = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "      filters = [64, 256, 512, 1024]\n",
    "    else:\n",
    "      raise ValueError('Unknown backbone')\n",
    "\n",
    "    # ENCODER\n",
    "    # Input: 3 x H x W ---> Output: 64 x H/2 x W/2\n",
    "    self.conv1 = backbone.conv1\n",
    "    self.bn1 = backbone.bn1\n",
    "    self.relu = backbone.relu\n",
    "\n",
    "    # Input: 64 x H/2 x W/2 --> Output: 64 x H/4 x W/4\n",
    "    self.maxpool = backbone.maxpool\n",
    "\n",
    "    # Input: 64 x H/4 x W/4 --> Output: 256 x H/4 x W/4\n",
    "    self.enc1 = backbone.layer1\n",
    "\n",
    "    #Input: 256 x H/4 x W/4 ---> Output: 512 x H/8 x W/8\n",
    "    self.enc2 = backbone.layer2\n",
    "\n",
    "    #Input: 512 x H/8 x W/8 ---> Output: 1024 x H/16 x W/16\n",
    "    self.enc3 = backbone.layer3\n",
    "\n",
    "    # Input: 1024 x H/16 x W/16 ---> Output: 2048 x H/32 x W/32\n",
    "    self.enc4 = backbone.layer4\n",
    "\n",
    "    # DECODER\n",
    "    # Output: 512 x H/16 x W/16\n",
    "    self.decoder1 = DecoderBlock(filters[3]*2, filters[3], dropout=dropout)\n",
    "\n",
    "    # Output: 256 x H/8 x W/8\n",
    "    self.decoder2 = DecoderBlock(filters[3], filters[2], dropout=dropout)\n",
    "\n",
    "    # Output: 128 x H/4 x W/4\n",
    "    self.decoder3 = DecoderBlock(filters[2], filters[1], dropout=dropout)\n",
    "\n",
    "    # Output: 64 x H/2 x W/2\n",
    "    self.decoder4 = DecoderBlock(filters[1], filters[0], dropout=dropout, in_conv=128)\n",
    "\n",
    "\n",
    "    # Input: 64 x H/2 x W/2, Output: n_classes x H x W\n",
    "    self.last = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=2, stride=2, padding=0),\n",
    "            nn.Conv2d(64, n_classes, kernel_size=3, padding=1)\n",
    "    )\n",
    "\n",
    "  def forward(self, inputs):\n",
    "\n",
    "    e0 = self.conv1(inputs)\n",
    "    e0 = self.bn1(e0)\n",
    "    e0 = self.relu(e0)\n",
    "    x = self.maxpool(e0)\n",
    "\n",
    "    # Encoder\n",
    "    enc1 = self.enc1(x)\n",
    "    enc2 = self.enc2(enc1)\n",
    "    enc3 = self.enc3(enc2)\n",
    "    enc4 = self.enc4(enc3)\n",
    "\n",
    "    # Decoder\n",
    "    dec1 = self.decoder1(enc4, enc3)\n",
    "    dec2 = self.decoder2(dec1, enc2)\n",
    "    dec3 = self.decoder3(dec2, enc1)\n",
    "    dec4 = self.decoder4(dec3, e0)\n",
    "    out = self.last(dec4)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2030,
     "status": "ok",
     "timestamp": 1762889851627,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "onb1cEAcNId4",
    "outputId": "2aa4fe6e-2c22-44aa-9284-e58931607e93"
   },
   "outputs": [],
   "source": [
    "unet = UNetResNet(NUM_CLASSES).to(device)\n",
    "# summary(unet, input_size=(3, 512, 512))\n",
    "input_tensor = torch.randn(1, 3, 512, 512).to(device)\n",
    "flops, params = profile(unet, inputs=(input_tensor, ))\n",
    "print(f\"FLOPs: {flops/1e9:.2f} GFLOPs\")\n",
    "print(f\"Params: {params/1e6:.2f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1326,
     "status": "ok",
     "timestamp": 1762889852958,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "0kAMd6xhYnbc",
    "outputId": "67a0a763-8285-46c2-e334-cb1e1ab29586"
   },
   "outputs": [],
   "source": [
    "unet = UNetResNet(NUM_CLASSES, backbone='resnet50').to(device)\n",
    "# summary(unet, input_size=(3, 512, 512))\n",
    "input_tensor = torch.randn(1, 3, 512, 512).to(device)\n",
    "flops, params = profile(unet, inputs=(input_tensor, ))\n",
    "print(f\"FLOPs: {flops/1e9:.2f} GFLOPs\")\n",
    "print(f\"Params: {params/1e6:.2f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzboc-NSKr2l"
   },
   "source": [
    "### DeepLab V3+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBuJNzeyNNqS"
   },
   "source": [
    "#### Dilated Convolution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Sof3Lk-NQnd"
   },
   "outputs": [],
   "source": [
    "class AtrousConvolution(nn.Module):\n",
    "\n",
    "  def __init__(self, in_c, out_c, kernel_size, pad, dilation_rate):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.conv = nn.Conv2d(in_channels=in_c, out_channels=out_c, kernel_size=kernel_size, dilation=dilation_rate, padding=pad)\n",
    "    self.bn = nn.BatchNorm2d(out_c)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    x = self.conv(x)\n",
    "    x = self.bn(x)\n",
    "    x = self.relu(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCqEeIlXNSOI"
   },
   "source": [
    "#### ASPP module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhuNA_qLNTvu"
   },
   "outputs": [],
   "source": [
    "class ASPP(nn.Module):\n",
    "\n",
    "  def __init__(self, in_c, out_c):\n",
    "    super().__init__()\n",
    "\n",
    "    self.conv1 = AtrousConvolution(in_c, out_c, kernel_size=1, dilation_rate=1, pad=0)\n",
    "\n",
    "    self.conv6 = AtrousConvolution(in_c, out_c, kernel_size=3, dilation_rate=6, pad=6)\n",
    "\n",
    "    self.conv12 = AtrousConvolution(in_c, out_c, kernel_size=3, dilation_rate=12, pad=12)\n",
    "\n",
    "    self.conv18 = AtrousConvolution(in_c, out_c, kernel_size=3, dilation_rate=18, pad=18)\n",
    "\n",
    "    self.image_pool = nn.Sequential(\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=1, stride=1, padding=0, dilation=1),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.final_conv = nn.Sequential(\n",
    "        nn.Conv2d(out_c * 5, out_c, kernel_size=1, stride=1, padding=0, dilation=1),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    x_1 = self.conv1(x)\n",
    "    x_6 = self.conv6(x)\n",
    "    x_12 = self.conv12(x)\n",
    "    x_18 = self.conv18(x)\n",
    "    x_pooled = self.image_pool(x)\n",
    "\n",
    "    x_pooled = F.interpolate(x_pooled, size=x_18.size()[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "    merged = torch.cat((x_1, x_6, x_12, x_18, x_pooled), dim=1)\n",
    "    x_final = self.final_conv(merged)\n",
    "\n",
    "    return x_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOny1GDNNVbO"
   },
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjcsUvgvNWsR"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "  def __init__(self, num_classes):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.conv1 = nn.Conv2d(256, 48, kernel_size=1, stride=1, padding=0)\n",
    "    self.bn1 = nn.BatchNorm2d(48)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "    self.last_conv = nn.Sequential(\n",
    "        nn.Conv2d(304, 256, kernel_size=3, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.1),\n",
    "    )\n",
    "    self.classifier = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "  def forward(self, low_level_features, x):\n",
    "\n",
    "    low_level = self.conv1(low_level_features)\n",
    "    low_level = self.bn1(low_level)\n",
    "    low_level = self.relu(low_level)\n",
    "    x = torch.cat((x, low_level), dim=1)\n",
    "    x = self.last_conv(x)\n",
    "    x = F.interpolate(x, scale_factor=(4,4), mode='bilinear', align_corners=True)\n",
    "    x = self.classifier(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rT5OcgArNZAp"
   },
   "source": [
    "#### Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2UDDZSFNa2a"
   },
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "\n",
    "     def __init__(self, backbone_name='resnet101', low_level='layer1', high_level='layer4', pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        assert low_level in ['conv1', 'layer1', 'layer2', 'layer3', 'layer4'], \\\n",
    "            f\"Invalid low_level '{low_level}', choose from ['conv1', 'layer1', 'layer2', 'layer3', 'layer4']\"\n",
    "\n",
    "        # To create a total stride of 16 in the backbone\n",
    "        dilation = [False, False, True]\n",
    "\n",
    "        if backbone_name == 'resnet50':\n",
    "            backbone = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT if pretrained else None,\n",
    "                                                   replace_stride_with_dilation=dilation)\n",
    "        elif backbone_name == 'resnet101':\n",
    "            backbone = torchvision.models.resnet101(weights=torchvision.models.ResNet101_Weights.DEFAULT if pretrained else None,\n",
    "                                                    replace_stride_with_dilation=dilation)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone_name}\")\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            backbone.conv1,\n",
    "            backbone.bn1,\n",
    "            backbone.relu\n",
    "        )\n",
    "        self.maxpool = backbone.maxpool\n",
    "        self.layer1 = backbone.layer1\n",
    "        self.layer2 = backbone.layer2\n",
    "        self.layer3 = backbone.layer3\n",
    "        self.layer4 = backbone.layer4\n",
    "\n",
    "        self.low_level = low_level\n",
    "        self.high_level = high_level\n",
    "\n",
    "     def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        l1 = self.layer1(x)\n",
    "        l2 = self.layer2(l1)\n",
    "        l3 = self.layer3(l2)\n",
    "        l4 = self.layer4(l3)\n",
    "\n",
    "        level_dict = {\n",
    "            'conv1': x,\n",
    "            'layer1': l1,\n",
    "            'layer2': l2,\n",
    "            'layer3': l3,\n",
    "            'layer4': l4,\n",
    "        }\n",
    "        low_level_feat = level_dict[self.low_level]\n",
    "\n",
    "        high_level_feat = level_dict[self.high_level]\n",
    "\n",
    "        return low_level_feat, high_level_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0k5fGMpNePT"
   },
   "source": [
    "#### Final architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dljoxCq6NgTo"
   },
   "outputs": [],
   "source": [
    "class DeepLabV3Plus(nn.Module):\n",
    "\n",
    "   def __init__(self, n_classes, backbone='resnet101', low_level='layer1', high_level='layer4', pretrained=True):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.backbone = Backbone(backbone_name=backbone, low_level=low_level, pretrained=pretrained, high_level=high_level)\n",
    "    self.aspp = ASPP(2048, 256)\n",
    "\n",
    "    self.decoder = Decoder(n_classes)\n",
    "\n",
    "   def forward(self, inputs):\n",
    "\n",
    "    low_level, high_level = self.backbone(inputs)\n",
    "    aspp = self.aspp(high_level)\n",
    "    aspp = F.interpolate(aspp, scale_factor=(4,4), mode='bilinear', align_corners=True)\n",
    "    out = self.decoder(low_level, aspp)\n",
    "\n",
    "    return out\n",
    "\n",
    "# unet = DeepLabV3Plus(NUM_CLASSES, 'resnet101', high_level='layer4').to(device)\n",
    "# summary(unet, input_size=(3, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 658,
     "status": "ok",
     "timestamp": 1762889853747,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "a4TNzeFCZ_x3",
    "outputId": "eb265d7c-8248-43fc-c409-287001cdf171"
   },
   "outputs": [],
   "source": [
    "unet = DeepLabV3Plus(NUM_CLASSES, 'resnet50', high_level='layer4').to(device)\n",
    "input_tensor = torch.randn(1, 3, 512, 512).to(device)\n",
    "flops, params = profile(unet, inputs=(input_tensor, ))\n",
    "print(f\"FLOPs: {flops/1e9:.2f} GFLOPs\")\n",
    "print(f\"Params: {params/1e6:.2f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1042,
     "status": "ok",
     "timestamp": 1762889854794,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "Du5njQ2dZjZd",
    "outputId": "30eea0c1-456d-4ccd-82af-4475c7904217"
   },
   "outputs": [],
   "source": [
    "unet = DeepLabV3Plus(NUM_CLASSES, 'resnet101', high_level='layer4').to(device)\n",
    "input_tensor = torch.randn(1, 3, 512, 512).to(device)\n",
    "flops, params = profile(unet, inputs=(input_tensor, ))\n",
    "print(f\"FLOPs: {flops/1e9:.2f} GFLOPs\")\n",
    "print(f\"Params: {params/1e6:.2f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSepfToIKukR"
   },
   "source": [
    "### Pytorch's DeepLab V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3750,
     "status": "ok",
     "timestamp": 1762889858546,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "bCdWnJ9QSuOv",
    "outputId": "2f026104-818b-4d4d-efbf-d142427cebd0"
   },
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import DeepLabV3_ResNet101_Weights, DeepLabV3_ResNet50_Weights\n",
    "\n",
    "class DeepLabV3Pytorch(nn.Module):\n",
    "\n",
    "  def __init__(self, num_classes, backbone='resnet101'):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    if backbone == 'resnet50':\n",
    "      self.model = torchvision.models.segmentation.deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.DEFAULT)\n",
    "    elif backbone == 'resnet101':\n",
    "      self.model = torchvision.models.segmentation.deeplabv3_resnet101(weights=DeepLabV3_ResNet101_Weights.DEFAULT)\n",
    "    else:\n",
    "      raise ValueError('Backbone not recognized')\n",
    "\n",
    "    self.model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    x = self.model(x)['out']\n",
    "\n",
    "    return x\n",
    "\n",
    "unet = DeepLabV3Pytorch(NUM_CLASSES, 'resnet101').to(device)\n",
    "input_tensor = torch.randn(1, 3, 512, 512).to(device)\n",
    "flops, params = profile(unet, inputs=(input_tensor, ))\n",
    "print(f\"FLOPs: {flops/1e9:.2f} GFLOPs\")\n",
    "print(f\"Params: {params/1e6:.2f} M\")\n",
    "\n",
    "unet = DeepLabV3Pytorch(NUM_CLASSES, 'resnet50').to(device)\n",
    "input_tensor = torch.randn(1, 3, 512, 512).to(device)\n",
    "flops, params = profile(unet, inputs=(input_tensor, ))\n",
    "print(f\"FLOPs: {flops/1e9:.2f} GFLOPs\")\n",
    "print(f\"Params: {params/1e6:.2f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X49ds9iDVHwh"
   },
   "source": [
    "## Training procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcw25fo1VJnA"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0, mode='max', verbose=True):\n",
    "\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, current_score, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "            self.best_weights = deepcopy(model.state_dict())\n",
    "        else:\n",
    "            improvement = (\n",
    "                (self.mode == 'min' and current_score < self.best_score - self.min_delta) or\n",
    "                (self.mode == 'max' and current_score > self.best_score + self.min_delta)\n",
    "            )\n",
    "            if improvement:\n",
    "                self.best_score = current_score\n",
    "                self.best_weights = deepcopy(model.state_dict())\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.verbose:\n",
    "                    print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "                if self.counter >= self.patience:\n",
    "                    if self.verbose:\n",
    "                        print(\"Early stopping triggered.\")\n",
    "                    self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUKlUb2NVUph"
   },
   "outputs": [],
   "source": [
    "def train_loop(model, num_epochs, optimizer, scheduler, criterion, metric, early_stop, device, train_loader, val_loader=None):\n",
    "\n",
    "  train_losses = []\n",
    "  val_losses = []\n",
    "  val_metric = []\n",
    "\n",
    "  global_step = 0\n",
    "\n",
    "  for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "\n",
    "    model.train()\n",
    "    train_loss_epoch = []\n",
    "    for idx, (images, masks) in enumerate(train_loader):\n",
    "      # Stack the list of tensors into a single tensor\n",
    "      images = torch.stack(images, dim=0).to(device)\n",
    "      masks = torch.stack(masks, dim=0).to(device)\n",
    "\n",
    "      predicted_mask = model(images)\n",
    "      loss = criterion(predicted_mask, masks.squeeze(1))\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss_epoch.append(loss.item())\n",
    "\n",
    "      # if idx % 10 == 0:\n",
    "      #   wandb.log({\n",
    "      #     \"train/loss\": loss\n",
    "      #   }, global_step)\n",
    "      #   global_step += 10\n",
    "    train_loss = np.mean(train_loss_epoch)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    if scheduler is not None:\n",
    "      scheduler.step(loss)\n",
    "\n",
    "    if val_loader is not None:\n",
    "      model.eval()\n",
    "      val_loss_epoch = []\n",
    "      val_metric_epoch = []\n",
    "      with torch.no_grad():\n",
    "        for idx, (images, masks) in enumerate(val_loader):\n",
    "          # Stack the list of tensors into a single tensor\n",
    "          images = torch.stack(images, dim=0).to(device)\n",
    "          masks = torch.stack(masks, dim=0).to(device)\n",
    "\n",
    "          predictions = model(images)\n",
    "          loss = criterion(predictions, masks.squeeze(1))\n",
    "          _, metric_value = metric(predictions, masks.squeeze(1))\n",
    "          val_loss_epoch.append(loss.item())\n",
    "          val_metric_epoch.append(metric_value.item())\n",
    "\n",
    "      new_loss_ep = np.mean(val_loss_epoch)\n",
    "      new_metric_ep = np.mean(val_metric_epoch)\n",
    "\n",
    "      val_losses.append(new_loss_ep)\n",
    "      val_metric.append(new_metric_ep)\n",
    "      # wandb.log({\n",
    "      #       \"val/loss\": np.mean(val_loss_epoch),\n",
    "      #       \"val/metric\": np.mean(val_metric_epoch)\n",
    "      #   })\n",
    "      print(f\"Epoch {epoch}: training loss {train_loss} -- validation metric {new_metric_ep} -- validation loss {new_loss_ep}\")\n",
    "      early_stop(new_metric_ep, model)\n",
    "      if early_stop.early_stop:\n",
    "        print(f\"Stopped early with best mAP: {early_stop.best_score:.2f}\")\n",
    "        break\n",
    "\n",
    "  return train_losses, val_losses, val_metric, early_stop.best_weights\n",
    "\n",
    "\n",
    "def train(model, num_epochs, optimizer, criterion, metric, scheduler, early_stop, device, project_name, run_name, train_loader, val_loader=None):\n",
    "\n",
    "  # wandb.init(project=project_name, name=run_name)\n",
    "\n",
    "  train_losses, val_losses, val_metric, best_model_state = train_loop(model, num_epochs, optimizer, scheduler, criterion, metric, early_stop, device, train_loader, val_loader)\n",
    "\n",
    "  # wandb.finish()\n",
    "\n",
    "  path_ckpts = Path(\"/content/drive/My Drive/CamVid/ckpts\")\n",
    "  path_ckpts.mkdir(exist_ok=True)\n",
    "  torch.save(best_model_state, path_ckpts / f\"{run_name}.pt\")\n",
    "\n",
    "  return train_losses, val_losses, val_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuZkTpSgVq96"
   },
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32Vfqy29euZy"
   },
   "outputs": [],
   "source": [
    "wandb_proj_name = \"CamVid2\"\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaB8s2dbdZbL"
   },
   "outputs": [],
   "source": [
    "train_data = CamVidDataset(NEW_TRAIN_PATH, NEW_TRAIN_LABELS, 512, 512, transforms=get_train_transform())\n",
    "val_data = CamVidDataset(VALID_PATH, LABEL_VALID_PATH, 512, 512, transforms=get_valid_transform())\n",
    "test_data = CamVidDataset(NEW_TEST_PATH, NEW_TEST_LABELS, 512, 512, transforms=get_valid_transform())\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCfwYQQdGSXc"
   },
   "source": [
    "### First comparison of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIoVJJWxGUZq"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 60\n",
    "BATCH_SIZE = 16\n",
    "WEIGHT_DECAY = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSEVdhZidVLW"
   },
   "source": [
    "#### VGG Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5myMQUvudXeM"
   },
   "outputs": [],
   "source": [
    "unet = UNet(NUM_CLASSES).to(device)\n",
    "lr=1e-4\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=1e-4, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "early_stop = EarlyStopping(patience=10, min_delta=0.001, mode='max')\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "run_name = f\"unet_AdamW_LR_{lr}_eps_{NUM_EPOCHS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1249305,
     "status": "ok",
     "timestamp": 1762425194604,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "UYJHhK-TfRnn",
    "outputId": "9dacc9e1-6c69-4bd7-c8c9-0177a36c5a5b"
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_metric = train(unet, NUM_EPOCHS, optimizer, criterion, multiclass_iou, scheduler, early_stop, device, wandb_proj_name, run_name, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22428,
     "status": "ok",
     "timestamp": 1762425843079,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "W4Vnaq_PfVfk",
    "outputId": "1915bb87-0c66-4edf-e85d-2d4e31f08532"
   },
   "outputs": [],
   "source": [
    "plot_curves(train_losses, val_losses, val_metric)\n",
    "unet = UNet(NUM_CLASSES).to(device)\n",
    "path_ckpts = Path(\"/content/drive/My Drive/CamVid/ckpts\")\n",
    "run_name = \"unet_AdamW_LR_0.0001_eps_60\"\n",
    "weight_path = path_ckpts / f\"{run_name}.pt\"\n",
    "unet.load_state_dict(torch.load(weight_path, map_location=torch.device(device)))\n",
    "evaluate_model(unet, test_loader, device, num_classes=11)\n",
    "evaluate_model(unet, val_loader, device, num_classes=11)\n",
    "evaluate_model(unet, train_loader, device, num_classes=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2150,
     "status": "ok",
     "timestamp": 1762425926372,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "ZWEJY8Gyw8qW",
    "outputId": "e6549749-4caf-4cca-fa22-a4d37d00ae49"
   },
   "outputs": [],
   "source": [
    "indexes = [0,20,50,80,99]\n",
    "\n",
    "unet.eval()\n",
    "with torch.no_grad():\n",
    "  for i in indexes:\n",
    "    img, mask = test_data[i]\n",
    "    img = img.unsqueeze(0)\n",
    "    pred = unet(img.to(device))\n",
    "    pred_classes = pred.argmax(dim=1)\n",
    "    visualize(img.squeeze(0), mask, predicted=pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SEPJiALaI0W"
   },
   "source": [
    "#### From scratch ResNet-34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9OtyrWTaL4a"
   },
   "outputs": [],
   "source": [
    "unet34 = UNetResNet34(NUM_CLASSES, pretrained=False).to(device)\n",
    "\n",
    "base_lr = 1e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(unet34.parameters(), lr=base_lr, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "early_stop = EarlyStopping(patience=10, min_delta=0.001, mode='max')\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "run_name = f\"scratch_unet34_AdamW_LR_{base_lr}_eps_{NUM_EPOCHS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1252071,
     "status": "ok",
     "timestamp": 1762891142215,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "UIOPzKbxaVYs",
    "outputId": "3328c759-2a92-419d-c990-ac2fe08cf188"
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_metric = train(unet34, NUM_EPOCHS, optimizer, criterion, multiclass_iou, scheduler, early_stop, device, wandb_proj_name, run_name, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 64727,
     "status": "ok",
     "timestamp": 1762891206972,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "8SKDPAaUaajq",
    "outputId": "e37cba4f-ee03-496c-8301-2f95eb78fb01"
   },
   "outputs": [],
   "source": [
    "plot_curves(train_losses, val_losses, val_metric)\n",
    "evaluate_model(unet34, test_loader, device, num_classes=11)\n",
    "evaluate_model(unet34, val_loader, device, num_classes=11)\n",
    "evaluate_model(unet34, train_loader, device, num_classes=11)\n",
    "\n",
    "indexes = [0,20,50,80,99]\n",
    "\n",
    "unet34.eval()\n",
    "with torch.no_grad():\n",
    "  for i in indexes:\n",
    "    img, mask = test_data[i]\n",
    "    img = img.unsqueeze(0)\n",
    "    pred = unet34(img.to(device))\n",
    "    pred_classes = pred.argmax(dim=1)\n",
    "    visualize(img.squeeze(0), mask, predicted=pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycZNS_Hyf66Z"
   },
   "source": [
    "#### Unet ResNet-34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 559,
     "status": "ok",
     "timestamp": 1762425965003,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "Phg-fC7ef_di",
    "outputId": "9f6da84b-bdc9-48a3-d2c1-7c90769009a7"
   },
   "outputs": [],
   "source": [
    "unet34 = UNetResNet34(NUM_CLASSES).to(device)\n",
    "\n",
    "base_lr = 1e-4\n",
    "\n",
    "for layer in [unet34.conv1, unet34.bn1, unet34.relu, unet34.maxpool]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "param_groups = [\n",
    "    {\"params\": unet34.enc1.parameters(), \"lr\": base_lr * 0.01},\n",
    "    {\"params\": unet34.enc2.parameters(), \"lr\": base_lr * 0.2},\n",
    "    {\"params\": unet34.enc3.parameters(), \"lr\": base_lr * 0.25},\n",
    "    {\"params\": unet34.enc4.parameters(), \"lr\": base_lr * 0.5},\n",
    "    {\"params\": unet34.bottleneck.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet34.decoder1.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet34.decoder2.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet34.decoder3.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet34.decoder4.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet34.decoder5.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet34.last.parameters(), \"lr\": base_lr},\n",
    "]\n",
    "\n",
    "print_trainable_params(unet34)\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=base_lr, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "early_stop = EarlyStopping(patience=10, min_delta=0.001, mode='max')\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "run_name = f\"unet34_AdamW_LR_{base_lr}_eps_{NUM_EPOCHS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 928
    },
    "executionInfo": {
     "elapsed": 1136201,
     "status": "ok",
     "timestamp": 1762427105402,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "4gei9EzFgCNT",
    "outputId": "847508dc-4009-4009-e940-e55b477092c8"
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_metric = train(unet34, NUM_EPOCHS, optimizer, criterion, multiclass_iou, scheduler, early_stop, device, wandb_proj_name, run_name, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 22893,
     "status": "ok",
     "timestamp": 1762427128716,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "UVOwPX16gDdh",
    "outputId": "c859f004-eb10-43c8-9038-172de4f13c29"
   },
   "outputs": [],
   "source": [
    "plot_curves(train_losses, val_losses, val_metric)\n",
    "evaluate_model(unet34, test_loader, device, num_classes=11)\n",
    "evaluate_model(unet34, val_loader, device, num_classes=11)\n",
    "evaluate_model(unet34, train_loader, device, num_classes=11)\n",
    "\n",
    "indexes = [0,20,50,80,99]\n",
    "\n",
    "unet34.eval()\n",
    "with torch.no_grad():\n",
    "  for i in indexes:\n",
    "    img, mask = test_data[i]\n",
    "    img = img.unsqueeze(0)\n",
    "    pred = unet34(img.to(device))\n",
    "    pred_classes = pred.argmax(dim=1)\n",
    "    visualize(img.squeeze(0), mask, predicted=pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ab0Dr0qbjOzV"
   },
   "source": [
    "#### Unet ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 804,
     "status": "ok",
     "timestamp": 1762427234898,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "E8vshXGHjar7",
    "outputId": "bd4d84c1-1f64-4b70-9dfe-bc71567243bb"
   },
   "outputs": [],
   "source": [
    "unet50 = UNetResNet(NUM_CLASSES, backbone='resnet50').to(device)\n",
    "\n",
    "base_lr = 1e-4\n",
    "\n",
    "for layer in [unet50.conv1, unet50.bn1, unet50.relu, unet50.maxpool]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "param_groups = [\n",
    "    {\"params\": unet50.enc1.parameters(), \"lr\": base_lr * 0.01},\n",
    "    {\"params\": unet50.enc2.parameters(), \"lr\": base_lr * 0.2},\n",
    "    {\"params\": unet50.enc3.parameters(), \"lr\": base_lr * 0.25},\n",
    "    {\"params\": unet50.enc4.parameters(), \"lr\": base_lr * 0.5},\n",
    "    {\"params\": unet50.decoder1.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet50.decoder2.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet50.decoder3.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet50.decoder4.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet50.last.parameters(), \"lr\": base_lr},\n",
    "]\n",
    "\n",
    "print_trainable_params(unet50)\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=base_lr, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "early_stop = EarlyStopping(patience=10, min_delta=0.001, mode='max')\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "run_name = f\"unet50_AdamW_LR_{base_lr}_eps_{NUM_EPOCHS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 997
    },
    "executionInfo": {
     "elapsed": 1157514,
     "status": "ok",
     "timestamp": 1762428400478,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "9kvkz-RejcBr",
    "outputId": "113153a8-ab3e-4914-e1ba-248514a31a8d"
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_metric = train(unet50, NUM_EPOCHS, optimizer, criterion, multiclass_iou, scheduler, early_stop, device, wandb_proj_name, run_name, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 23676,
     "status": "ok",
     "timestamp": 1762428424181,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "wgYCpBCjjdTN",
    "outputId": "7cebe94c-c278-4770-9579-478a5c69e3a5"
   },
   "outputs": [],
   "source": [
    "plot_curves(train_losses, val_losses, val_metric)\n",
    "evaluate_model(unet50, test_loader, device, num_classes=11)\n",
    "evaluate_model(unet50, val_loader, device, num_classes=11)\n",
    "evaluate_model(unet50, train_loader, device, num_classes=11)\n",
    "\n",
    "indexes = [0,20,50,80,99]\n",
    "\n",
    "unet50.eval()\n",
    "with torch.no_grad():\n",
    "  for i in indexes:\n",
    "    img, mask = test_data[i]\n",
    "    img = img.unsqueeze(0)\n",
    "    pred = unet50(img.to(device))\n",
    "    pred_classes = pred.argmax(dim=1)\n",
    "    visualize(img.squeeze(0), mask, predicted=pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1RRuPo_jSZA"
   },
   "source": [
    "#### Unet ResNet-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1225,
     "status": "ok",
     "timestamp": 1762428447893,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "D1ac3nHwkUcD",
    "outputId": "e1ef3b02-39f9-4b84-e80f-a7889ca5050c"
   },
   "outputs": [],
   "source": [
    "unet101 = UNetResNet(NUM_CLASSES, backbone='resnet101').to(device)\n",
    "\n",
    "base_lr = 1e-4\n",
    "\n",
    "for layer in [unet101.conv1, unet101.bn1, unet101.relu, unet101.maxpool]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "param_groups = [\n",
    "    {\"params\": unet101.enc1.parameters(), \"lr\": base_lr * 0.01},\n",
    "    {\"params\": unet101.enc2.parameters(), \"lr\": base_lr * 0.2},\n",
    "    {\"params\": unet101.enc3.parameters(), \"lr\": base_lr * 0.25},\n",
    "    {\"params\": unet101.enc4.parameters(), \"lr\": base_lr * 0.5},\n",
    "    {\"params\": unet101.decoder1.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet101.decoder2.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet101.decoder3.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet101.decoder4.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet101.last.parameters(), \"lr\": base_lr},\n",
    "]\n",
    "\n",
    "print_trainable_params(unet101)\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=base_lr, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "early_stop = EarlyStopping(patience=10, min_delta=0.001, mode='max')\n",
    "critection = nn.CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "run_name = f\"unet101_AdamW_LR_{base_lr}_eps_{NUM_EPOCHS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 997
    },
    "executionInfo": {
     "elapsed": 1160612,
     "status": "ok",
     "timestamp": 1762429611225,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "bGQHejQkkWOE",
    "outputId": "62c5d3e6-0e26-4218-e6d1-cbd520d6f85b"
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_metric = train(unet101, NUM_EPOCHS, optimizer, criterion, multiclass_iou, scheduler, early_stop, device, wandb_proj_name, run_name, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 23832,
     "status": "ok",
     "timestamp": 1762429635071,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "9fqxrfvDkXgT",
    "outputId": "546d2cd2-02e9-4e94-8cda-7e33fa150d30"
   },
   "outputs": [],
   "source": [
    "plot_curves(train_losses, val_losses, val_metric)\n",
    "evaluate_model(unet101, test_loader, device, num_classes=11)\n",
    "evaluate_model(unet101, val_loader, device, num_classes=11)\n",
    "evaluate_model(unet101, train_loader, device, num_classes=11)\n",
    "\n",
    "indexes = [0,20,50,80,99]\n",
    "\n",
    "unet101.eval()\n",
    "with torch.no_grad():\n",
    "  for i in indexes:\n",
    "    img, mask = test_data[i]\n",
    "    img = img.unsqueeze(0)\n",
    "    pred = unet101(img.to(device))\n",
    "    pred_classes = pred.argmax(dim=1)\n",
    "    visualize(img.squeeze(0), mask, predicted=pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix4vMoZ2AJQQ"
   },
   "source": [
    "#### DeepLab V3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 594,
     "status": "ok",
     "timestamp": 1762430292476,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "U_78YMicALDm",
    "outputId": "4f57b728-17ba-4990-c14c-dd24bc7e8c4a"
   },
   "outputs": [],
   "source": [
    "deeplab = DeepLabV3Plus(NUM_CLASSES, backbone='resnet50').to(device)\n",
    "\n",
    "base_lr = 1e-4\n",
    "\n",
    "for layer in [deeplab.backbone.conv1, deeplab.backbone.maxpool]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "param_groups = [\n",
    "    {\"params\": deeplab.backbone.layer1.parameters(), \"lr\": base_lr * 0.01},\n",
    "    {\"params\": deeplab.backbone.layer2.parameters(), \"lr\": base_lr * 0.2},\n",
    "    {\"params\": deeplab.backbone.layer3.parameters(), \"lr\": base_lr * 0.25},\n",
    "    {\"params\": deeplab.backbone.layer4.parameters(), \"lr\": base_lr * 0.5},\n",
    "    {\"params\": deeplab.aspp.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": deeplab.decoder.parameters(), \"lr\": base_lr}\n",
    "]\n",
    "\n",
    "print_trainable_params(deeplab)\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=base_lr, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "early_stop = EarlyStopping(patience=10, min_delta=0.001, mode='max')\n",
    "critection = nn.CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "run_name = f\"deeplab_AdamW_LR_{base_lr}_eps_{NUM_EPOCHS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 136708,
     "status": "ok",
     "timestamp": 1762431636851,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "gWc40bkEBRFY",
    "outputId": "c1357fdf-4de4-4d1d-b0f6-24266212ec5e"
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_metric = train(deeplab, NUM_EPOCHS, optimizer, criterion, multiclass_iou, scheduler, early_stop, device, wandb_proj_name, run_name, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 24238,
     "status": "ok",
     "timestamp": 1762431662011,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "7om2FRSGBtRT",
    "outputId": "e69c0bb0-edc7-4bd0-822e-62f2f0976074"
   },
   "outputs": [],
   "source": [
    "plot_curves(train_losses, val_losses, val_metric)\n",
    "evaluate_model(deeplab, test_loader, device, num_classes=11)\n",
    "evaluate_model(deeplab, val_loader, device, num_classes=11)\n",
    "evaluate_model(deeplab, train_loader, device, num_classes=11)\n",
    "\n",
    "indexes = [0,20,50,80,99]\n",
    "\n",
    "deeplab.eval()\n",
    "with torch.no_grad():\n",
    "  for i in indexes:\n",
    "    img, mask = test_data[i]\n",
    "    img = img.unsqueeze(0)\n",
    "    pred = deeplab(img.to(device))\n",
    "    pred_classes = pred.argmax(dim=1)\n",
    "    visualize(img.squeeze(0), mask, predicted=pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2JpLpuIb1c-"
   },
   "source": [
    "#### DeepLab V3+ 50 from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ut-D_1Qqb5_N"
   },
   "outputs": [],
   "source": [
    "deeplab = DeepLabV3Plus(NUM_CLASSES, backbone='resnet50', pretrained=False).to(device)\n",
    "\n",
    "base_lr = 1e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(deeplab.parameters(), lr=base_lr, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "early_stop = EarlyStopping(patience=10, min_delta=0.001, mode='max')\n",
    "critection = nn.CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "run_name = f\"scratch_deeplab_AdamW_LR_{base_lr}_eps_{NUM_EPOCHS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 744445,
     "status": "ok",
     "timestamp": 1762893142132,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "_Y06EBgLcJcT",
    "outputId": "7c5c9422-39a2-458a-eccc-2d72e78128df"
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_metric = train(deeplab, NUM_EPOCHS, optimizer, criterion, multiclass_iou, scheduler, early_stop, device, wandb_proj_name, run_name, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 23975,
     "status": "ok",
     "timestamp": 1762893166128,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "oJTNd-K1cLor",
    "outputId": "18612a85-28a3-4a59-809c-67bef3892f71"
   },
   "outputs": [],
   "source": [
    "plot_curves(train_losses, val_losses, val_metric)\n",
    "evaluate_model(deeplab, test_loader, device, num_classes=11)\n",
    "evaluate_model(deeplab, val_loader, device, num_classes=11)\n",
    "evaluate_model(deeplab, train_loader, device, num_classes=11)\n",
    "\n",
    "indexes = [0,20,50,80,99]\n",
    "\n",
    "deeplab.eval()\n",
    "with torch.no_grad():\n",
    "  for i in indexes:\n",
    "    img, mask = test_data[i]\n",
    "    img = img.unsqueeze(0)\n",
    "    pred = deeplab(img.to(device))\n",
    "    pred_classes = pred.argmax(dim=1)\n",
    "    visualize(img.squeeze(0), mask, predicted=pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biBzSYk8CXYV"
   },
   "source": [
    "#### DeepLab V3+ 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 924,
     "status": "ok",
     "timestamp": 1762431685688,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "DLDf7_hBCd1U",
    "outputId": "3a31aae3-4227-4a6e-9d5f-97218dcc6c53"
   },
   "outputs": [],
   "source": [
    "deeplab101 = DeepLabV3Plus(NUM_CLASSES, backbone='resnet101').to(device)\n",
    "\n",
    "base_lr = 1e-4\n",
    "\n",
    "for layer in [deeplab101.backbone.conv1, deeplab101.backbone.maxpool]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "param_groups = [\n",
    "    {\"params\": deeplab101.backbone.layer1.parameters(), \"lr\": base_lr * 0.01},\n",
    "    {\"params\": deeplab101.backbone.layer2.parameters(), \"lr\": base_lr * 0.2},\n",
    "    {\"params\": deeplab101.backbone.layer3.parameters(), \"lr\": base_lr * 0.25},\n",
    "    {\"params\": deeplab101.backbone.layer4.parameters(), \"lr\": base_lr * 0.5},\n",
    "    {\"params\": deeplab101.aspp.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": deeplab101.decoder.parameters(), \"lr\": base_lr}\n",
    "]\n",
    "\n",
    "print_trainable_params(deeplab101)\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=base_lr, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "early_stop = EarlyStopping(patience=10, min_delta=0.001, mode='max')\n",
    "critection = nn.CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "run_name = f\"deeplab101_AdamW_LR_{base_lr}_eps_{NUM_EPOCHS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 367288,
     "status": "ok",
     "timestamp": 1762432867182,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "h1jnUlQ5Ct8F",
    "outputId": "8a513d57-65df-4124-cd23-e44ea6992d76"
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_metric = train(deeplab101, NUM_EPOCHS, optimizer, criterion, multiclass_iou, scheduler, early_stop, device, wandb_proj_name, run_name, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 24992,
     "status": "ok",
     "timestamp": 1762432892219,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "QH7eoEFxCxdW",
    "outputId": "19e57e6a-0807-4629-94ce-3d6633a0b80f"
   },
   "outputs": [],
   "source": [
    "plot_curves(train_losses, val_losses, val_metric)\n",
    "evaluate_model(deeplab101, test_loader, device, num_classes=11)\n",
    "evaluate_model(deeplab101, val_loader, device, num_classes=11)\n",
    "evaluate_model(deeplab101, train_loader, device, num_classes=11)\n",
    "\n",
    "indexes = [0,20,50,80,99]\n",
    "\n",
    "deeplab101.eval()\n",
    "with torch.no_grad():\n",
    "  for i in indexes:\n",
    "    img, mask = test_data[i]\n",
    "    img = img.unsqueeze(0)\n",
    "    pred = deeplab101(img.to(device))\n",
    "    pred_classes = pred.argmax(dim=1)\n",
    "    visualize(img.squeeze(0), mask, predicted=pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAbEQ3v9IZaO"
   },
   "source": [
    "#### Pretrained DeepLab V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 835,
     "status": "ok",
     "timestamp": 1762433781792,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "6V1QHvubIdHR",
    "outputId": "b989a857-77f8-4994-f461-015c7c648b9b"
   },
   "outputs": [],
   "source": [
    "pytorchdeeplab = DeepLabV3Pytorch(NUM_CLASSES).to(device)\n",
    "\n",
    "base_lr = 1e-4\n",
    "\n",
    "for name, param in pytorchdeeplab.model.backbone.named_parameters():\n",
    "       if not any(f in name for f in [\"layer2\", \"layer3\", \"layer4\"]):\n",
    "            param.requires_grad = False\n",
    "classifier_params = []\n",
    "backbone_params = []\n",
    "for name, param in pytorchdeeplab.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        continue\n",
    "    elif name.startswith(\"classifier\"):\n",
    "        classifier_params.append(param)\n",
    "    elif name.startswith(\"aux_classifier\"):\n",
    "        continue\n",
    "    else:\n",
    "        backbone_params.append(param)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': backbone_params, 'lr': base_lr * 0.1, },   # smaller LR for pretrained backbone\n",
    "    {'params': classifier_params, 'lr': base_lr}       # main head: larger LR\n",
    "], weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "early_stop = EarlyStopping(patience=10, min_delta=0.001, mode='max')\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "print_trainable_params(pytorchdeeplab)\n",
    "\n",
    "run_name = f\"pytorch_deeplab101_AdamW_LR_{base_lr}_eps_{NUM_EPOCHS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "1a-Kixk8JtjX",
    "outputId": "207ee8d6-2ad9-4820-f9cd-e54fadea4d4b"
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_metric = train(pytorchdeeplab, NUM_EPOCHS, optimizer, criterion, multiclass_iou, scheduler, early_stop, device, wandb_proj_name, run_name, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVU36FukJvGi"
   },
   "outputs": [],
   "source": [
    "plot_curves(train_losses, val_losses, val_metric)\n",
    "evaluate_model(pytorchdeeplab, test_loader, device, num_classes=11)\n",
    "evaluate_model(pytorchdeeplab, val_loader, device, num_classes=11)\n",
    "evaluate_model(pytorchdeeplab, train_loader, device, num_classes=11)\n",
    "\n",
    "indexes = [0,20,50,80,99]\n",
    "\n",
    "pytorchdeeplab.eval()\n",
    "with torch.no_grad():\n",
    "  for i in indexes:\n",
    "    img, mask = test_data[i]\n",
    "    img = img.unsqueeze(0)\n",
    "    pred = pytorchdeeplab(img.to(device))\n",
    "    pred_classes = pred.argmax(dim=1)\n",
    "    visualize(img.squeeze(0), mask, predicted=pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KPJC5lkTH4R"
   },
   "source": [
    "#### Pretrained DeepLab V3\n",
    "\n",
    "Higher lr is used because otherwise the convergence was too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1095,
     "status": "ok",
     "timestamp": 1762435359878,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "oQkaYXygTJwn",
    "outputId": "7e172bd8-65c9-42bc-a26d-f27c9c86a5f5"
   },
   "outputs": [],
   "source": [
    "pytorchdeeplab = DeepLabV3Pytorch(NUM_CLASSES).to(device)\n",
    "\n",
    "base_lr = 1e-3\n",
    "\n",
    "for name, param in pytorchdeeplab.model.backbone.named_parameters():\n",
    "       if not any(f in name for f in [\"layer2\", \"layer3\", \"layer4\"]):\n",
    "            param.requires_grad = False\n",
    "classifier_params = []\n",
    "backbone_params = []\n",
    "for name, param in pytorchdeeplab.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        continue\n",
    "    elif name.startswith(\"classifier\"):\n",
    "        classifier_params.append(param)\n",
    "    elif name.startswith(\"aux_classifier\"):\n",
    "        continue\n",
    "    else:\n",
    "        backbone_params.append(param)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': backbone_params, 'lr': base_lr * 0.1, },   # smaller LR for pretrained backbone\n",
    "    {'params': classifier_params, 'lr': base_lr}       # main head: larger LR\n",
    "], weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "early_stop = EarlyStopping(patience=10, min_delta=0.001, mode='max')\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "print_trainable_params(pytorchdeeplab)\n",
    "\n",
    "run_name = f\"pytorch_deeplab101_AdamW_LR_{base_lr}_eps_{NUM_EPOCHS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "CrG3JWlBTPS_",
    "outputId": "53ba8cdf-174e-4a0c-d9dc-915d051d2d4a"
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_metric = train(pytorchdeeplab, NUM_EPOCHS, optimizer, criterion, multiclass_iou, scheduler, early_stop, device, wandb_proj_name, run_name, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23606,
     "status": "ok",
     "timestamp": 1762436962900,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "4nfx-lB0TQtW",
    "outputId": "7873c6e3-0a68-4800-a738-8a96691e4a5a"
   },
   "outputs": [],
   "source": [
    "# plot_curves(train_losses, val_losses, val_metric)\n",
    "pytorchdeeplab = DeepLabV3Pytorch(NUM_CLASSES).to(device)\n",
    "path_ckpts = Path(\"/content/drive/My Drive/CamVid/ckpts\")\n",
    "run_name = \"pytorch_deeplab101_AdamW_LR_0.001_eps_60\"\n",
    "weight_path = path_ckpts / f\"{run_name}.pt\"\n",
    "pytorchdeeplab.load_state_dict(torch.load(weight_path, map_location=torch.device(device)))\n",
    "evaluate_model(pytorchdeeplab, test_loader, device, num_classes=11)\n",
    "evaluate_model(pytorchdeeplab, val_loader, device, num_classes=11)\n",
    "evaluate_model(pytorchdeeplab, train_loader, device, num_classes=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2116,
     "status": "ok",
     "timestamp": 1762437006514,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "BfHvl-62bW7W",
    "outputId": "097f7dee-2186-4924-92ec-255b29850cc6"
   },
   "outputs": [],
   "source": [
    "indexes = [0,20,50,80,99]\n",
    "\n",
    "pytorchdeeplab.eval()\n",
    "with torch.no_grad():\n",
    "  for i in indexes:\n",
    "    img, mask = test_data[i]\n",
    "    img = img.unsqueeze(0)\n",
    "    pred = pytorchdeeplab(img.to(device))\n",
    "    pred_classes = pred.argmax(dim=1)\n",
    "    visualize(img.squeeze(0), mask, predicted=pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3F2bgN-GdiQ"
   },
   "source": [
    "### Change loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvYsBYR3HyMw"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 60\n",
    "WEIGHT_DECAY = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Eyyecq2py_L"
   },
   "source": [
    "#### Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17409,
     "status": "ok",
     "timestamp": 1762845358935,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "HiZzey8Cp2Yc",
    "outputId": "698cd26c-1695-4c50-d41b-b08bb93d0c0d"
   },
   "outputs": [],
   "source": [
    "num_classes = 11\n",
    "class_counts = torch.zeros(num_classes)\n",
    "\n",
    "for _, targets in train_loader:\n",
    "    targets = torch.stack(targets, dim=0)\n",
    "    targets = targets.squeeze(1)\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        mask = (targets == cls) & (targets != 255)\n",
    "        class_counts[cls] += mask.sum().item()\n",
    "\n",
    "# Normalizing with max\n",
    "class_weights = 1.0 / (class_counts + 1e-6)\n",
    "class_weights = class_weights / class_weights.max()\n",
    "\n",
    "# Normalizing with sum\n",
    "# class_weights = 1.0 / (class_counts + 1e-6)\n",
    "# class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "# Median frequency balancing\n",
    "# frequencies = class_counts / class_counts.sum()\n",
    "# median_freq = torch.median(frequencies[frequencies > 0])\n",
    "# class_weights = median_freq / (frequencies + 1e-6)\n",
    "\n",
    "CLASS_WEIGHTS = torch.cat([class_weights, torch.tensor([0.0])])\n",
    "for i in range(len(CLASS_WEIGHTS)):\n",
    "  print(f\"Weights for class {CAMVID_CLASSES[i]} is : {CLASS_WEIGHTS[i]:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdjS9Dw9Gv6z"
   },
   "source": [
    "#### Unet ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 826,
     "status": "ok",
     "timestamp": 1762507636287,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "-azlU2clG0-B",
    "outputId": "55fe21e5-fb2c-445c-a955-0e292cdc7984"
   },
   "outputs": [],
   "source": [
    "unet50 = UNetResNet(NUM_CLASSES, backbone='resnet50').to(device)\n",
    "\n",
    "base_lr = 1e-4\n",
    "\n",
    "for layer in [unet50.conv1, unet50.bn1, unet50.relu, unet50.maxpool]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "param_groups = [\n",
    "    {\"params\": unet50.enc1.parameters(), \"lr\": base_lr * 0.01},\n",
    "    {\"params\": unet50.enc2.parameters(), \"lr\": base_lr * 0.2},\n",
    "    {\"params\": unet50.enc3.parameters(), \"lr\": base_lr * 0.25},\n",
    "    {\"params\": unet50.enc4.parameters(), \"lr\": base_lr * 0.5},\n",
    "    {\"params\": unet50.decoder1.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet50.decoder2.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet50.decoder3.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet50.decoder4.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet50.last.parameters(), \"lr\": base_lr},\n",
    "]\n",
    "\n",
    "print_trainable_params(unet50)\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=base_lr, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "early_stop = EarlyStopping(patience=10, min_delta=0.001, mode='max')\n",
    "\n",
    "# Ignore index 11 because of internal problems of FocalLoss - solve it!\n",
    "criterion = FocalLoss(alpha=CLASS_WEIGHTS, ignore_index=11)\n",
    "\n",
    "run_name = f\"focal_unet50_AdamW_LR_{base_lr}_eps_{NUM_EPOCHS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1755669,
     "status": "ok",
     "timestamp": 1762509394734,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "OakxKgUOHKb8",
    "outputId": "a8154a71-6816-47f2-fc12-c2163d6cd199"
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_metric = train(unet50, NUM_EPOCHS, optimizer, criterion, multiclass_iou, scheduler, early_stop, device, wandb_proj_name, run_name, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 69531,
     "status": "ok",
     "timestamp": 1762509464295,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "IClDk6ECHGJQ",
    "outputId": "8b3f2e08-dee6-48a4-a4db-1e01c9cd12db"
   },
   "outputs": [],
   "source": [
    "plot_curves(train_losses, val_losses, val_metric)\n",
    "evaluate_model(unet50, test_loader, device, num_classes=11)\n",
    "evaluate_model(unet50, val_loader, device, num_classes=11)\n",
    "evaluate_model(unet50, train_loader, device, num_classes=11)\n",
    "\n",
    "indexes = [0,20,50,80,99]\n",
    "\n",
    "unet50.eval()\n",
    "with torch.no_grad():\n",
    "  for i in indexes:\n",
    "    img, mask = test_data[i]\n",
    "    img = img.unsqueeze(0)\n",
    "    pred = unet50(img.to(device))\n",
    "    pred_classes = pred.argmax(dim=1)\n",
    "    visualize(img.squeeze(0), mask, predicted=pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gc31LxFo90J1"
   },
   "source": [
    "#### UNet ResNet-50 median alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17113,
     "status": "ok",
     "timestamp": 1762848770022,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "aVWcGH_D97s6",
    "outputId": "9174a50b-2f3f-4282-bfc6-c174dfaa37a4"
   },
   "outputs": [],
   "source": [
    "num_classes = 11\n",
    "class_counts = torch.zeros(num_classes)\n",
    "\n",
    "for _, targets in train_loader:\n",
    "    targets = torch.stack(targets, dim=0)\n",
    "    targets = targets.squeeze(1)\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        mask = (targets == cls) & (targets != 255)\n",
    "        class_counts[cls] += mask.sum().item()\n",
    "\n",
    "# Normalizing with sum\n",
    "# class_weights = 1.0 / (class_counts + 1e-6)\n",
    "# class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "# Median frequency balancing\n",
    "frequencies = class_counts / class_counts.sum()\n",
    "median_freq = torch.median(frequencies[frequencies > 0])\n",
    "class_weights_median = median_freq / (frequencies + 1e-6)\n",
    "\n",
    "CLASS_WEIGHTS_MEDIAN = torch.cat([class_weights_median, torch.tensor([0.0])])\n",
    "for i in range(len(CLASS_WEIGHTS_MEDIAN)):\n",
    "  print(f\"Weights for class {CAMVID_CLASSES[i]} is : {CLASS_WEIGHTS_MEDIAN[i]:.3f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1425,
     "status": "ok",
     "timestamp": 1762848976364,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "9J60BwEX-JnB",
    "outputId": "fb29ee6e-57f8-4a6c-eb6c-5033d88f518f"
   },
   "outputs": [],
   "source": [
    "unet50 = UNetResNet(NUM_CLASSES, backbone='resnet50').to(device)\n",
    "\n",
    "base_lr = 1e-4\n",
    "\n",
    "for layer in [unet50.conv1, unet50.bn1, unet50.relu, unet50.maxpool]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "param_groups = [\n",
    "    {\"params\": unet50.enc1.parameters(), \"lr\": base_lr * 0.01},\n",
    "    {\"params\": unet50.enc2.parameters(), \"lr\": base_lr * 0.2},\n",
    "    {\"params\": unet50.enc3.parameters(), \"lr\": base_lr * 0.25},\n",
    "    {\"params\": unet50.enc4.parameters(), \"lr\": base_lr * 0.5},\n",
    "    {\"params\": unet50.decoder1.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet50.decoder2.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet50.decoder3.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet50.decoder4.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": unet50.last.parameters(), \"lr\": base_lr},\n",
    "]\n",
    "\n",
    "print_trainable_params(unet50)\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=base_lr, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "early_stop = EarlyStopping(patience=10, min_delta=0.001, mode='max')\n",
    "\n",
    "# Ignore index 11 because of internal problems of FocalLoss - solve it!\n",
    "criterion = FocalLoss(alpha=CLASS_WEIGHTS_MEDIAN, ignore_index=11)\n",
    "\n",
    "run_name = f\"focal_median_unet50_AdamW_LR_{base_lr}_eps_{NUM_EPOCHS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1675397,
     "status": "ok",
     "timestamp": 1762850834447,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "GfXXhma8-RH8",
    "outputId": "ac414c3a-efc2-4bef-ef50-ef359062f64f"
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_metric = train(unet50, NUM_EPOCHS, optimizer, criterion, multiclass_iou, scheduler, early_stop, device, wandb_proj_name, run_name, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 25888,
     "status": "ok",
     "timestamp": 1762850860364,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "FguNIh-e-ZDO",
    "outputId": "993e288f-f884-4bac-d516-4f03053e9de7"
   },
   "outputs": [],
   "source": [
    "plot_curves(train_losses, val_losses, val_metric)\n",
    "evaluate_model(unet50, test_loader, device, num_classes=11)\n",
    "evaluate_model(unet50, val_loader, device, num_classes=11)\n",
    "evaluate_model(unet50, train_loader, device, num_classes=11)\n",
    "\n",
    "indexes = [0,20,50,80,99]\n",
    "\n",
    "unet50.eval()\n",
    "with torch.no_grad():\n",
    "  for i in indexes:\n",
    "    img, mask = test_data[i]\n",
    "    img = img.unsqueeze(0)\n",
    "    pred = unet50(img.to(device))\n",
    "    pred_classes = pred.argmax(dim=1)\n",
    "    visualize(img.squeeze(0), mask, predicted=pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUl3oVI_HVPq"
   },
   "source": [
    "#### DeepLab V3+ ResNet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIRHM3_rxS7r"
   },
   "outputs": [],
   "source": [
    "train_data = CamVidDataset(NEW_TRAIN_PATH, NEW_TRAIN_LABELS, 512, 512, transforms=get_train_transform())\n",
    "val_data = CamVidDataset(VALID_PATH, LABEL_VALID_PATH, 512, 512, transforms=get_valid_transform())\n",
    "test_data = CamVidDataset(NEW_TEST_PATH, NEW_TEST_LABELS, 512, 512, transforms=get_valid_transform())\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_data, batch_size=8, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=8, shuffle=False, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1902,
     "status": "ok",
     "timestamp": 1762845374258,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "WJ087lV-JCVs",
    "outputId": "a6befecb-3cf4-45fb-814b-98dfe0d6d511"
   },
   "outputs": [],
   "source": [
    "deeplab101 = DeepLabV3Plus(NUM_CLASSES, backbone='resnet101').to(device)\n",
    "\n",
    "base_lr = 1e-4\n",
    "\n",
    "for layer in [deeplab101.backbone.conv1, deeplab101.backbone.maxpool]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "param_groups = [\n",
    "    {\"params\": deeplab101.backbone.layer1.parameters(), \"lr\": base_lr * 0.01},\n",
    "    {\"params\": deeplab101.backbone.layer2.parameters(), \"lr\": base_lr * 0.2},\n",
    "    {\"params\": deeplab101.backbone.layer3.parameters(), \"lr\": base_lr * 0.25},\n",
    "    {\"params\": deeplab101.backbone.layer4.parameters(), \"lr\": base_lr * 0.5},\n",
    "    {\"params\": deeplab101.aspp.parameters(), \"lr\": base_lr},\n",
    "    {\"params\": deeplab101.decoder.parameters(), \"lr\": base_lr}\n",
    "]\n",
    "\n",
    "print_trainable_params(deeplab101)\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=base_lr, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "early_stop = EarlyStopping(patience=10, min_delta=0.001, mode='max')\n",
    "criterion = FocalLoss(alpha=CLASS_WEIGHTS, ignore_index=11)\n",
    "\n",
    "run_name = f\"focal_deeplab101_AdamW_LR_{base_lr}_eps_{NUM_EPOCHS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2236805,
     "status": "ok",
     "timestamp": 1762847616660,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "TrIwIEcyJHwU",
    "outputId": "a7505f2f-dfcb-4ede-c0e1-76b99d39b59a"
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_metric = train(deeplab101, NUM_EPOCHS, optimizer, criterion, multiclass_iou, scheduler, early_stop, device, wandb_proj_name, run_name, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 44645,
     "status": "ok",
     "timestamp": 1762847661421,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "M_pvGXxQJKtW",
    "outputId": "e4c264cf-cd79-4ec9-82c6-451a9ad2fcfb"
   },
   "outputs": [],
   "source": [
    "plot_curves(train_losses, val_losses, val_metric)\n",
    "evaluate_model(deeplab101, test_loader, device, num_classes=11)\n",
    "evaluate_model(deeplab101, val_loader, device, num_classes=11)\n",
    "evaluate_model(deeplab101, train_loader, device, num_classes=11)\n",
    "\n",
    "indexes = [0,20,50,80,99]\n",
    "\n",
    "deeplab101.eval()\n",
    "with torch.no_grad():\n",
    "  for i in indexes:\n",
    "    img, mask = test_data[i]\n",
    "    img = img.unsqueeze(0)\n",
    "    pred = deeplab101(img.to(device))\n",
    "    pred_classes = pred.argmax(dim=1)\n",
    "    visualize(img.squeeze(0), mask, predicted=pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tkgBbSKIA2S"
   },
   "source": [
    "## Final visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVClH8NqIC0m"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compare_models_visualization(models: dict,\n",
    "                                 test_dataset,\n",
    "                                 device: str = \"cuda\",\n",
    "                                 num_samples: int = 3,\n",
    "                                 void_label: int = 255):\n",
    "    \"\"\"\n",
    "    Visualize predictions from multiple models side-by-side.\n",
    "\n",
    "    Args:\n",
    "        models (dict): Dictionary {model_name: model_object}\n",
    "        test_dataset: Dataset object returning (image, target)\n",
    "        device (str): 'cuda' or 'cpu'\n",
    "        num_samples (int): Number of random samples to visualize\n",
    "        void_label (int): Label ID for void pixels (ignored)\n",
    "    \"\"\"\n",
    "\n",
    "    # Set models to eval mode\n",
    "    for m in models.values():\n",
    "        m.eval()\n",
    "\n",
    "    # Randomly sample images\n",
    "    indices = random.sample(range(len(test_dataset)), num_samples)\n",
    "    n_models = len(models)\n",
    "\n",
    "    for idx in indices:\n",
    "        image, target = test_dataset[idx]\n",
    "        image = image.to(device).unsqueeze(0)  # (1, C, H, W)\n",
    "        target = target.squeeze().cpu()\n",
    "\n",
    "        # Create figure\n",
    "        n_cols = 2 + n_models  # image + GT + predictions\n",
    "        plt.figure(figsize=(5 * n_cols, 4))\n",
    "\n",
    "        plt.subplots_adjust(left=0, right=1, top=1, bottom=0, wspace=0.01, hspace=0.01)\n",
    "\n",
    "        # Show input image\n",
    "        img_disp = image.squeeze().cpu()\n",
    "        denormalize(img_disp)\n",
    "        plt.subplot(1, n_cols, 1)\n",
    "        plt.imshow(img_disp.permute(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Input Image\", fontsize=14, weight=\"bold\")\n",
    "\n",
    "        # Show ground truth\n",
    "        plt.subplot(1, n_cols, 2)\n",
    "        plt.imshow(colorize_mask(target))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Ground Truth\", fontsize=14, weight=\"bold\")\n",
    "\n",
    "        # Model predictions\n",
    "        for i, (name, model) in enumerate(models.items(), start=3):\n",
    "            preds = model(image)\n",
    "            if isinstance(preds, dict) and \"out\" in preds:\n",
    "                preds = preds[\"out\"]  # handle torchvision models\n",
    "            pred_mask = preds.argmax(dim=1).squeeze().cpu()\n",
    "\n",
    "            plt.subplot(1, n_cols, i)\n",
    "            plt.imshow(colorize_mask(pred_mask))\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"{name} Prediction\", fontsize=14)\n",
    "\n",
    "        plt.subplots_adjust(left=0, right=1, top=1, bottom=0, wspace=0.02, hspace=0.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "executionInfo": {
     "elapsed": 53232,
     "status": "ok",
     "timestamp": 1762868838064,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "W_CRFo5kIxX1",
    "outputId": "08fae49f-058d-409d-ded6-febba47df1f9"
   },
   "outputs": [],
   "source": [
    "baseline = UNet(NUM_CLASSES).to(device)\n",
    "path_ckpts = Path(\"/content/drive/My Drive/CamVid/ckpts\")\n",
    "run_name = \"unet_AdamW_LR_0.0001_eps_60\"\n",
    "weight_path = path_ckpts / f\"{run_name}.pt\"\n",
    "baseline.load_state_dict(torch.load(weight_path, map_location=torch.device(device)))\n",
    "resnet34 = UNetResNet34(NUM_CLASSES).to(device)\n",
    "run_name = \"unet34_AdamW_LR_0.0001_eps_60\"\n",
    "weight_path = path_ckpts / f\"{run_name}.pt\"\n",
    "resnet34.load_state_dict(torch.load(weight_path, map_location=torch.device(device)))\n",
    "resnet50 = UNetResNet(NUM_CLASSES, backbone='resnet50').to(device)\n",
    "run_name = \"unet50_AdamW_LR_0.0001_eps_60\"\n",
    "weight_path = path_ckpts / f\"{run_name}.pt\"\n",
    "resnet50.load_state_dict(torch.load(weight_path, map_location=torch.device(device)))\n",
    "deeplab_custom = DeepLabV3Plus(NUM_CLASSES, backbone='resnet101').to(device)\n",
    "run_name = \"deeplab101_AdamW_LR_0.0001_eps_60\"\n",
    "weight_path = path_ckpts / f\"{run_name}.pt\"\n",
    "deeplab_custom.load_state_dict(torch.load(weight_path, map_location=torch.device(device)))\n",
    "deeplab_torch = DeepLabV3Pytorch(NUM_CLASSES).to(device)\n",
    "run_name = \"pytorch_deeplab101_AdamW_LR_0.001_eps_60\"\n",
    "weight_path = path_ckpts / f\"{run_name}.pt\"\n",
    "deeplab_torch.load_state_dict(torch.load(weight_path, map_location=torch.device(device)))\n",
    "focal_unet50 = UNetResNet(NUM_CLASSES, backbone='resnet50').to(device)\n",
    "run_name = \"focal_unet50_AdamW_LR_0.0001_eps_60\"\n",
    "weight_path = path_ckpts / f\"{run_name}.pt\"\n",
    "focal_unet50.load_state_dict(torch.load(weight_path, map_location=torch.device(device)))\n",
    "\n",
    "models = {\n",
    "    \"Baseline\": baseline,\n",
    "    \"U-Net (ResNet-34)\": resnet34,\n",
    "    \"U-Net (ResNet-50)\": resnet50,\n",
    "    \"DeepLabV3+\": deeplab_custom,\n",
    "    \"DeepLabV3 (Torch)\": deeplab_torch,\n",
    "    \"Focal U-Net (ResNet-50)\": focal_unet50\n",
    "}\n",
    "\n",
    "compare_models_visualization(models, test_data, device=\"cuda\", num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1XwgO09eTleAx1IM5WDHdP2hcCXCNPLH9"
    },
    "executionInfo": {
     "elapsed": 16164,
     "status": "ok",
     "timestamp": 1762870742649,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "lCbE8TTvNlzd",
    "outputId": "d2aaa250-e27f-420f-f1fd-edc3256cd86a"
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Baseline\": baseline,\n",
    "    \"U-Net (ResNet-34)\": resnet34,\n",
    "    \"U-Net (ResNet-50)\": resnet50,\n",
    "    \"DeepLabV3+\": deeplab_custom,\n",
    "    \"DeepLabV3 (Torch)\": deeplab_torch,\n",
    "    \"Focal U-Net (ResNet-50)\": focal_unet50\n",
    "}\n",
    "\n",
    "compare_models_visualization(models, test_data, device=\"cuda\", num_samples=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1fjBz5nTh21IWm50ONY0pWvjreKET6rrc"
    },
    "executionInfo": {
     "elapsed": 9443,
     "status": "ok",
     "timestamp": 1762870355457,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "2UyJowrjK3p8",
    "outputId": "9f1e201d-51b8-4716-a67c-33b40859ab74"
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Baseline\": baseline,\n",
    "    \"U-Net (ResNet-34)\": resnet34,\n",
    "    \"U-Net (ResNet-50)\": resnet50,\n",
    "    \"DeepLabV3+\": deeplab_custom,\n",
    "    \"DeepLabV3 (Torch)\": deeplab_torch,\n",
    "    \"Focal U-Net (ResNet-50)\": focal_unet50\n",
    "}\n",
    "\n",
    "compare_models_visualization(models, test_data, device=\"cuda\", num_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Trr5NWbGNFpZ"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compare_models_visualization_grid(models: dict,\n",
    "                                 test_dataset,\n",
    "                                 device: str = \"cuda\",\n",
    "                                 num_samples: int = 3,\n",
    "                                 void_label: int = 255):\n",
    "    \"\"\"\n",
    "    Visualize predictions from multiple models side-by-side (multi-row grid layout).\n",
    "\n",
    "    Each row = one random sample from the dataset.\n",
    "    Columns = Input | Ground Truth | Predictions for each model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set all models to evaluation mode\n",
    "    for m in models.values():\n",
    "        m.eval()\n",
    "\n",
    "    # Randomly select samples\n",
    "    indices = random.sample(range(len(test_dataset)), num_samples)\n",
    "    n_models = len(models)\n",
    "    n_cols = 2 + n_models  # image + GT + predictions\n",
    "\n",
    "    # Create figure grid\n",
    "    fig, axes = plt.subplots(num_samples, n_cols,\n",
    "                             figsize=(3.5 * n_cols, 3.5 * num_samples))\n",
    "\n",
    "    # Ensure axes is 2D even if num_samples = 1\n",
    "    if num_samples == 1:\n",
    "        axes = axes[None, :]\n",
    "\n",
    "    for row, idx in enumerate(indices):\n",
    "        image, target = test_dataset[idx]\n",
    "        image = image.to(device).unsqueeze(0)\n",
    "        target = target.squeeze().cpu()\n",
    "\n",
    "        # Input image\n",
    "        img_disp = image.squeeze().cpu()\n",
    "        denormalize(img_disp)\n",
    "        axes[row, 0].imshow(img_disp.permute(1, 2, 0))\n",
    "        axes[row, 0].axis(\"off\")\n",
    "        if row == 0:\n",
    "            axes[row, 0].set_title(\"Input\", fontsize=13, weight=\"bold\")\n",
    "\n",
    "        # Ground truth\n",
    "        axes[row, 1].imshow(colorize_mask(target))\n",
    "        axes[row, 1].axis(\"off\")\n",
    "        if row == 0:\n",
    "            axes[row, 1].set_title(\"Ground Truth\", fontsize=13, weight=\"bold\")\n",
    "\n",
    "        # Model predictions\n",
    "        for col, (name, model) in enumerate(models.items(), start=2):\n",
    "            preds = model(image)\n",
    "            if isinstance(preds, dict) and \"out\" in preds:\n",
    "                preds = preds[\"out\"]\n",
    "            pred_mask = preds.argmax(dim=1).squeeze().cpu()\n",
    "\n",
    "            axes[row, col].imshow(colorize_mask(pred_mask))\n",
    "            axes[row, col].axis(\"off\")\n",
    "            if row == 0:\n",
    "                axes[row, col].set_title(f\"{name} Prediction\", fontsize=12, pad=4)\n",
    "\n",
    "    # Tighten layout (almost no gaps)\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0, wspace=0.02, hspace=0.02)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1TIO3RWtyBzjJGY9kGS3cPohcUHCPvx8N"
    },
    "executionInfo": {
     "elapsed": 9356,
     "status": "ok",
     "timestamp": 1762869440900,
     "user": {
      "displayName": "Tomaž Cotič",
      "userId": "14572889724974508954"
     },
     "user_tz": -60
    },
    "id": "WUQUXiiHNHu9",
    "outputId": "edc17284-2c99-461b-d46b-be209d4b4cde"
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Baseline\": baseline,\n",
    "    \"U-Net (ResNet-34)\": resnet34,\n",
    "    \"U-Net (ResNet-50)\": resnet50,\n",
    "    \"DeepLabV3+\": deeplab_custom,\n",
    "    \"DeepLabV3 (Torch)\": deeplab_torch,\n",
    "    \"Focal U-Net (ResNet-50)\": focal_unet50\n",
    "}\n",
    "\n",
    "compare_models_visualization_grid(models, test_data, device=\"cuda\", num_samples=6)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOI6QfNwMwKDWGEw3Zl70iA",
   "collapsed_sections": [
    "vuMcf4yIDUGD",
    "xnpLiZvMDh9W",
    "8PV_QdLbDjR6",
    "S5ge4UoTG9Rj",
    "8KLwhWkEEISK",
    "mk49oVIqW_AQ",
    "Y-U850M5EFnL",
    "wK9gQrPqH66X",
    "jrsmvvYJ8S7M",
    "OC7ZruV7KUjb",
    "Jk9c2ZeZSJAM",
    "uBkBIxgoSMhL",
    "iBuJNzeyNNqS",
    "lCqEeIlXNSOI",
    "BOny1GDNNVbO",
    "rT5OcgArNZAp",
    "L0k5fGMpNePT",
    "rSepfToIKukR",
    "X49ds9iDVHwh",
    "KSEVdhZidVLW",
    "2SEPJiALaI0W",
    "ycZNS_Hyf66Z",
    "Ab0Dr0qbjOzV",
    "B1RRuPo_jSZA",
    "ix4vMoZ2AJQQ",
    "biBzSYk8CXYV",
    "WAbEQ3v9IZaO",
    "2KPJC5lkTH4R",
    "Q3F2bgN-GdiQ",
    "5Eyyecq2py_L",
    "IdjS9Dw9Gv6z",
    "Gc31LxFo90J1",
    "eUl3oVI_HVPq",
    "6tkgBbSKIA2S"
   ],
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
